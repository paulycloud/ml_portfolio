{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.\n",
    "\n",
    "The idea of using \"unfancy\" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt. Below are a few guidelines on how to engineer \"unfancy\" prompts.\n",
    "\n",
    "* Be concise\n",
    "* Be specific, and well-defined\n",
    "* Ask one task at a time\n",
    "* Improve response quality by including examples\n",
    "* Turn generative tasks to classification tasks to improve safety"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the envrionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install the vertex AI SDK \n",
    "!pip install google-cloud-aiplatform --upgrade --user \n",
    "\n",
    "## import libraries \n",
    "from vertexai.preview.language_models import TextGeneraionModel\n",
    "\n",
    "## set reusable functions \n",
    "def genModelPredict(prompt,tokens):\n",
    "    print(generation_model.predict(prompt=prompt,max_output_tokens=tokens).text)\n",
    "    \n",
    "## load the model\n",
    "generation_model = TextGeneraionModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples and different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general \n",
    "prompt = \"Suggest 5 names for a SaaS startup focusing on automated trading for everyone\"\n",
    "\n",
    "genModelPredict(prompt,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select from a list of options\n",
    "\n",
    "prompt = \"\"\"\n",
    "\n",
    "I'm a cloud consultant. Which technologies should i learn to be competitive in 2023 and why: \n",
    "a) learn Cryto \n",
    "b) learn AI & ML \n",
    "c) learn Quantum\n",
    "d) learn Cloud\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "genModelPredict(prompt,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorization \n",
    "\n",
    "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "\n",
    "Tweet: I loved the new YouTube video you made!\n",
    "Sentiment: positive\n",
    "\n",
    "Tweet: That was awful. Super boring ðŸ˜ \n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hallucinations \n",
    "\n",
    "prompt = \"Who was the first female president in USA? Only answer if you can factually back up this information\"\n",
    "\n",
    "genModelPredict(prompt,256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
