{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aW4BhG_PTvd"
      },
      "source": [
        "# UK and Ireland English speaker accent recognition using TFHub Transfer Learning \n",
        "**Author:** [Fadi Badine](https://twitter.com/fadibadine)<br>\n",
        "\n",
        "**Project Type:** TensorFlow Hub<br>\n",
        "\n",
        "**Date created:** 2022/04/16<br>\n",
        "\n",
        "**Last modified:** 2022/04/16<br>\n",
        "\n",
        "**Description:** Using TFHub Model Audio Event classifier called Yamnett Model, to classify UK & Ireland accents using feature extraction from [Yamnet Model](https://tfhub.dev/google/yamnet/1). \n",
        "\n",
        "**TF Original Model:** [Yamnet Model](https://tfhub.dev/google/yamnet/1) An audio event classifier trained on the AudioSet dataset to predict audio events from the AudioSet ontology.<br>\n",
        "\n",
        "**Training Dataset** Feature extraction and training will use the [Crowdsourced HQ UK &Ireland English Dialect Speeh Speech Data set](https://openslr.org/83/) which consists of a total of 17,877 audio wav files. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTgfdUawPTvf"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The following example shows how to use feature extraction in order to\n",
        "train a model to classify the English accent spoken in an audio wave.\n",
        "\n",
        "Instead of training a model from scratch, transfer learning enables us to\n",
        "take advantage of existing state-of-the-art deep learning models and use them as feature extractors.\n",
        "\n",
        "Our process:\n",
        "\n",
        "* Use a TF Hub pre-trained model (Yamnet) and apply it as part of the tf.data pipeline which transforms\n",
        "the audio files into feature vectors.\n",
        "* Train a dense model on the feature vectors.\n",
        "* Use the trained model for inference on a new audio file.\n",
        "\n",
        "### Steps: \n",
        "1. Configurations \n",
        "1. Imports \n",
        "1. Yamnet Model \n",
        "1. Dtaset \n",
        "1. Download the Data \n",
        "1. Load the Data in a dataframe \n",
        "1. Prepare training & Loss Sets\n",
        "1. Build the model \n",
        "1. Class weights calculation \n",
        "1. Callbacks \n",
        "1. Training \n",
        "1. Results \n",
        "1. Evaluation\n",
        "1. Confusion Matrix \n",
        "1. Precision & Recall\n",
        "1. Run inference on test data\n",
        "\n",
        "Note:\n",
        "\n",
        "* We need to install TensorFlow IO in order to resample audio files to 16 kHz as required by Yamnet model.\n",
        "* In the test section, ffmpeg is used to convert the mp3 file to wav.\n",
        "\n",
        "You can install TensorFlow IO with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX4Nmr0dPTvf",
        "outputId": "658ea063-b281-499e-9d9d-f7b7cd572087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 26.9 MB 58.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 46.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q tensorflow_io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRWaLgCLPTvg"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZwQRkYtlPTvg"
      },
      "outputs": [],
      "source": [
        "SEED = 1337\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_RATIO = 0.1\n",
        "MODEL_NAME = \"uk_irish_accent_recognition\"\n",
        "\n",
        "# Location where the dataset will be downloaded.\n",
        "# By default (None), keras.utils.get_file will use ~/.keras/ as the CACHE_DIR\n",
        "CACHE_DIR = None\n",
        "\n",
        "# The location of the dataset\n",
        "URL_PATH = \"https://www.openslr.org/resources/83/\"\n",
        "\n",
        "# List of datasets compressed files that contain the audio files\n",
        "zip_files = {\n",
        "    0: \"irish_english_male.zip\",\n",
        "    1: \"midlands_english_female.zip\",\n",
        "    2: \"midlands_english_male.zip\",\n",
        "    3: \"northern_english_female.zip\",\n",
        "    4: \"northern_english_male.zip\",\n",
        "    5: \"scottish_english_female.zip\",\n",
        "    6: \"scottish_english_male.zip\",\n",
        "    7: \"southern_english_female.zip\",\n",
        "    8: \"southern_english_male.zip\",\n",
        "    9: \"welsh_english_female.zip\",\n",
        "    10: \"welsh_english_male.zip\",\n",
        "}\n",
        "\n",
        "# We see that there are 2 compressed files for each accent (except Irish):\n",
        "# - One for male speakers\n",
        "# - One for female speakers\n",
        "# However, we will using a gender agnostic dataset.\n",
        "\n",
        "# List of gender agnostic categories\n",
        "gender_agnostic_categories = [\n",
        "    \"ir\",  # Irish\n",
        "    \"mi\",  # Midlands\n",
        "    \"no\",  # Northern\n",
        "    \"sc\",  # Scottish\n",
        "    \"so\",  # Southern\n",
        "    \"we\",  # Welsh\n",
        "]\n",
        "\n",
        "class_names = [\n",
        "    \"Irish\",\n",
        "    \"Midlands\",\n",
        "    \"Northern\",\n",
        "    \"Scottish\",\n",
        "    \"Southern\",\n",
        "    \"Welsh\",\n",
        "    \"Not a speech\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mull-evNPTvh"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OhXrRzL5PTvh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_io as tfio\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from IPython.display import Audio\n",
        "\n",
        "\n",
        "# Set all random seeds in order to get reproducible results\n",
        "keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# Where to download the dataset\n",
        "DATASET_DESTINATION = os.path.join(CACHE_DIR if CACHE_DIR else \"~/.keras/\", \"datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGFk0yrePTvh"
      },
      "source": [
        "## Yamnet Model\n",
        "\n",
        "Yamnet is an audio event classifier trained on the AudioSet dataset to predict audio\n",
        "events from the AudioSet ontology. It is available on TensorFlow Hub.\n",
        "\n",
        "Yamnet accepts a 1-D tensor of audio samples with a sample rate of 16 kHz.\n",
        "As output, the model returns a 3-tuple:\n",
        "\n",
        "* Scores of shape `(N, 521)` representing the scores of the 521 classes.\n",
        "* Embeddings of shape `(N, 1024)`.\n",
        "* The log-mel spectrogram of the entire audio frame.\n",
        "\n",
        "We will use the embeddings, which are the features extracted from the audio samples, as the input to our dense model.\n",
        "\n",
        "For more detailed information about Yamnet, please refer to its [TensorFlow Hub](https://tfhub.dev/google/yamnet/1) page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7XLQEL8bPTvh"
      },
      "outputs": [],
      "source": [
        "yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEPtmeaXPTvi"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset used is the\n",
        "[Crowdsourced high-quality UK and Ireland English Dialect speech data set](https://openslr.org/83/)\n",
        "which consists of a total of 17,877 high-quality audio wav files.\n",
        "\n",
        "This dataset includes over 31 hours of recording from 120 vounteers who self-identify as\n",
        "native speakers of Southern England, Midlands, Northern England, Wales, Scotland and Ireland.\n",
        "\n",
        "For more info, please refer to the above link or to the following paper:\n",
        "[Open-source Multi-speaker Corpora of the English Accents in the British Isles](https://aclanthology.org/2020.lrec-1.804.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju4eLwlxPTvi"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHylw7VDPTvi",
        "outputId": "e0c30ffa-8e7c-4114-f0e5-905b2f856fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.openslr.org/resources/83/line_index_all.csv\n",
            "1986139/1986139 [==============================] - 1s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/irish_english_male.zip\n",
            "164531638/164531638 [==============================] - 9s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/midlands_english_female.zip\n",
            "103085118/103085118 [==============================] - 6s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/midlands_english_male.zip\n",
            "166833961/166833961 [==============================] - 9s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/northern_english_female.zip\n",
            "314983063/314983063 [==============================] - 16s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/northern_english_male.zip\n",
            "817772034/817772034 [==============================] - 40s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/scottish_english_female.zip\n",
            "351443880/351443880 [==============================] - 18s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/scottish_english_male.zip\n",
            "620254118/620254118 [==============================] - 32s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/southern_english_female.zip\n",
            "1636701939/1636701939 [==============================] - 79s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/southern_english_male.zip\n",
            "1700955740/1700955740 [==============================] - 81s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/welsh_english_female.zip\n",
            "595683538/595683538 [==============================] - 29s 0us/step\n",
            "Downloading data from https://www.openslr.org/resources/83/welsh_english_male.zip\n",
            "757645790/757645790 [==============================] - 37s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# CSV file that contains information about the dataset. For each entry, we have:\n",
        "# - ID\n",
        "# - wav file name\n",
        "# - transcript\n",
        "line_index_file = keras.utils.get_file(\n",
        "    fname=\"line_index_file\", origin=URL_PATH + \"line_index_all.csv\"\n",
        ")\n",
        "\n",
        "# Download the list of compressed files that contains the audio wav files\n",
        "for i in zip_files:\n",
        "    fname = zip_files[i].split(\".\")[0]\n",
        "    url = URL_PATH + zip_files[i]\n",
        "\n",
        "    zip_file = keras.utils.get_file(fname=fname, origin=url, extract=True)\n",
        "    os.remove(zip_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gWnmejSPTvi"
      },
      "source": [
        "## Load the data in a Dataframe\n",
        "\n",
        "Of the 3 columns (ID, filename and transcript), we are only interested in the filename column in order to read the audio file.\n",
        "We will ignore the other two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nvLFic9fPTvi",
        "outputId": "29b48cfa-b745-4902-a95d-daf37f279082"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 filename\n",
              "0   wef_12484_01482829612\n",
              "1   wef_12484_01345932698\n",
              "2   wef_12484_00999757777\n",
              "3   wef_12484_00036278823\n",
              "4   wef_12484_00458512623"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1cd9c3d4-ff94-46f2-8c4c-3a16c5167a67\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wef_12484_01482829612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wef_12484_01345932698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wef_12484_00999757777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wef_12484_00036278823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wef_12484_00458512623</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1cd9c3d4-ff94-46f2-8c4c-3a16c5167a67')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1cd9c3d4-ff94-46f2-8c4c-3a16c5167a67 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1cd9c3d4-ff94-46f2-8c4c-3a16c5167a67');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataframe = pd.read_csv(\n",
        "    line_index_file, names=[\"id\", \"filename\", \"transcript\"], usecols=[\"filename\"]\n",
        ")\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OZ2sPaqPTvi"
      },
      "source": [
        "Let's now preprocess the dataset by:\n",
        "\n",
        "* Adjusting the filename (removing a leading space & adding \".wav\" extension to the\n",
        "filename).\n",
        "* Creating a label using the first 2 characters of the filename which indicate the\n",
        "accent.\n",
        "* Shuffling the samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PR8S99i1PTvi",
        "outputId": "db62562e-9539-4274-9c84-bdd58096d29d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      filename  label\n",
              "0  ~/.keras/datasets/som_03853_01027933689.wav      4\n",
              "1  ~/.keras/datasets/som_04310_01833253760.wav      4\n",
              "2  ~/.keras/datasets/sof_06136_01210700905.wav      4\n",
              "3  ~/.keras/datasets/som_02484_00261230384.wav      4\n",
              "4  ~/.keras/datasets/nom_06136_00616878975.wav      2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-faf25f12-3279-4854-a959-1177b13f1430\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>~/.keras/datasets/som_03853_01027933689.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>~/.keras/datasets/som_04310_01833253760.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>~/.keras/datasets/sof_06136_01210700905.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>~/.keras/datasets/som_02484_00261230384.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>~/.keras/datasets/nom_06136_00616878975.wav</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faf25f12-3279-4854-a959-1177b13f1430')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-faf25f12-3279-4854-a959-1177b13f1430 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-faf25f12-3279-4854-a959-1177b13f1430');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# The purpose of this function is to preprocess the dataframe by applying the following:\n",
        "# - Cleaning the filename from a leading space\n",
        "# - Generating a label column that is gender agnostic i.e.\n",
        "#   welsh english male and welsh english female for example are both labeled as\n",
        "#   welsh english\n",
        "# - Add extension .wav to the filename\n",
        "# - Shuffle samples\n",
        "def preprocess_dataframe(dataframe):\n",
        "    # Remove leading space in filename column\n",
        "    dataframe[\"filename\"] = dataframe.apply(lambda row: row[\"filename\"].strip(), axis=1)\n",
        "\n",
        "    # Create gender agnostic labels based on the filename first 2 letters\n",
        "    dataframe[\"label\"] = dataframe.apply(\n",
        "        lambda row: gender_agnostic_categories.index(row[\"filename\"][:2]), axis=1\n",
        "    )\n",
        "\n",
        "    # Add the file path to the name\n",
        "    dataframe[\"filename\"] = dataframe.apply(\n",
        "        lambda row: os.path.join(DATASET_DESTINATION, row[\"filename\"] + \".wav\"), axis=1\n",
        "    )\n",
        "\n",
        "    # Shuffle the samples\n",
        "    dataframe = dataframe.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "dataframe = preprocess_dataframe(dataframe)\n",
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVIg4LAlPTvj"
      },
      "source": [
        "## Prepare training & validation sets\n",
        "\n",
        "Let's split the samples creating training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haDQ84JsPTvj",
        "outputId": "c3ec2cfe-f4b3-4e54-f1b0-c42868a18602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 16089 training samples & 1788 validation ones\n"
          ]
        }
      ],
      "source": [
        "split = int(len(dataframe) * (1 - VALIDATION_RATIO))\n",
        "train_df = dataframe[:split]\n",
        "valid_df = dataframe[split:]\n",
        "\n",
        "print(\n",
        "    f\"We have {train_df.shape[0]} training samples & {valid_df.shape[0]} validation ones\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hOZYJyPPTvj"
      },
      "source": [
        "## Prepare a TensorFlow Dataset\n",
        "\n",
        "Next, we need to create a `tf.data.Dataset`.\n",
        "This is done by creating a `dataframe_to_dataset` function that does the following:\n",
        "\n",
        "* Create a dataset using filenames and labels.\n",
        "* Get the Yamnet embeddings by calling another function `filepath_to_embeddings`.\n",
        "* Apply caching, reshuffling and setting batch size.\n",
        "\n",
        "The `filepath_to_embeddings` does the following:\n",
        "\n",
        "* Load audio file.\n",
        "* Resample audio to 16 kHz.\n",
        "* Generate scores and embeddings from Yamnet model.\n",
        "* Since Yamnet generates multiple samples for each audio file,\n",
        "this function also duplicates the label for all the generated samples\n",
        "that have `score=0` (speech) whereas sets the label for the others as\n",
        "'other' indicating that this audio segment is not a speech and we won't label it as one of the accents.\n",
        "\n",
        "The below `load_16k_audio_file` is copied from the following tutorial\n",
        "[Transfer learning with YAMNet for environmental sound classification](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "m7kBlNhXPTvj",
        "outputId": "a22edf3e-a0fa-40c4-dcad-aa957ace13d6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-88f14783509f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mvalid_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-88f14783509f>\u001b[0m in \u001b[0;36mdataframe_to_dataset\u001b[0;34m(dataframe, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m     dataset = dataset.map(\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfilepath_to_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     ).unbatch()\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2054\u001b[0m       \u001b[0mmap_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0melement\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mA\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5286\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5288\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_and_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5289\u001b[0m     variant_tensor = gen_dataset_ops.flat_map_dataset(\n\u001b[1;32m   5290\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2566\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2568\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2569\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0marg_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2533\u001b[0;31m       self._flat_input_signature = tuple(nest.flatten(input_signature,\n\u001b[0m\u001b[1;32m   2534\u001b[0m                                                       expand_composites=True))\n\u001b[1;32m   2535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2709\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marglen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arg_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m               \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arg_indices_to_default_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m           ]\n\u001b[1;32m   2713\u001b[0m           raise TypeError(f\"{self.signature_summary()} missing required \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m         \u001b[0mannotation_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fullargspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvarkw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m       \u001b[0mkwarg_annotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fullargspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mkwarg_annotation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_tensor_or_tensor_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 options=autograph.ConversionOptions(\n\u001b[0;32m-> 1141\u001b[0;31m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    246\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filebktlqchm.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtf__lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_function_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_to_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__lam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/core/function_wrappers.py\u001b[0m in \u001b[0;36mwith_function_scope\u001b[0;34m(thunk, scope_name, options)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;34m\"\"\"Inline version of the FunctionScope context manager.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mFunctionScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lambda_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/__autograph_generated_filebktlqchm.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(lscope)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtf__lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_function_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_to_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__lam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m       \u001b[0m_attach_error_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filepfltu6lq.py\u001b[0m in \u001b[0;36mtf__filepath_to_embeddings\u001b[0;34m(filename, label)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0maudio_wav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_16k_audio_wav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myamnet_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_wav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0membeddings_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileg27sjvn4.py\u001b[0m in \u001b[0;36mtf__load_16k_audio_wav\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0maudio_wav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_wav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0maudio_wav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_wav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/audio_ops.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(input, rate_in, rate_out, name)\u001b[0m\n\u001b[1;32m    460\u001b[0m         )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorized_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mg1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/audio_ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         return core_ops.io_audio_resample(\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrate_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrate_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attrb)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/__init__.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/__init__.py\u001b[0m in \u001b[0;36m_load_library\u001b[0;34m(filename, lib)\u001b[0m\n\u001b[1;32m     69\u001b[0m     raise NotImplementedError(\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"unable to open file: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;34m+\u001b[0m \u001b[0;34mf\"{filename}, from paths: {filenames}\\ncaused by: {errs}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"<ipython-input-10-e58ac20f9032>\", line 44, in None  *\n        lambda x, y: filepath_to_embeddings(x, y),\n    File \"<ipython-input-10-e58ac20f9032>\", line 19, in filepath_to_embeddings  *\n        audio_wav = load_16k_audio_wav(filename)\n    File \"<ipython-input-10-e58ac20f9032>\", line 12, in load_16k_audio_wav  *\n        audio_wav = tfio.audio.resample(audio_wav, rate_in=sample_rate, rate_out=16000)\n    File \"/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/audio_ops.py\", line 462, in resample  **\n        value = tf.vectorized_map(f, input)\n    File \"/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/audio_ops.py\", line 458, in f\n        return core_ops.io_audio_resample(\n    File \"/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/__init__.py\", line 88, in __getattr__\n        return getattr(self._load(), attrb)\n    File \"/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/__init__.py\", line 84, in _load\n        self._mod = _load_library(self._library)\n    File \"/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/__init__.py\", line 71, in _load_library\n        + f\"{filename}, from paths: {filenames}\\ncaused by: {errs}\"\n\n    NotImplementedError: unable to open file: libtensorflow_io.so, from paths: ['/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n    caused by: ['/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZN10tensorflow4data11DatasetBase8FinalizeEPNS_15OpKernelContextESt8functionIFNS_8StatusOrISt10unique_ptrIS1_NS_4core15RefCountDeleterEEEEvEE']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "@tf.function\n",
        "def load_16k_audio_wav(filename):\n",
        "    # Read file content\n",
        "    file_content = tf.io.read_file(filename)\n",
        "\n",
        "    # Decode audio wave\n",
        "    audio_wav, sample_rate = tf.audio.decode_wav(file_content, desired_channels=1)\n",
        "    audio_wav = tf.squeeze(audio_wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "\n",
        "    # Resample to 16k\n",
        "    audio_wav = tfio.audio.resample(audio_wav, rate_in=sample_rate, rate_out=16000)\n",
        "\n",
        "    return audio_wav\n",
        "\n",
        "\n",
        "def filepath_to_embeddings(filename, label):\n",
        "    # Load 16k audio wave\n",
        "    audio_wav = load_16k_audio_wav(filename)\n",
        "\n",
        "    # Get audio embeddings & scores.\n",
        "    # The embeddings are the audio features extracted using transfer learning\n",
        "    # while scores will be used to identify time slots that are not speech\n",
        "    # which will then be gathered into a specific new category 'other'\n",
        "    scores, embeddings, _ = yamnet_model(audio_wav)\n",
        "\n",
        "    # Number of embeddings in order to know how many times to repeat the label\n",
        "    embeddings_num = tf.shape(embeddings)[0]\n",
        "    labels = tf.repeat(label, embeddings_num)\n",
        "\n",
        "    # Change labels for time-slots that are not speech into a new category 'other'\n",
        "    labels = tf.where(tf.argmax(scores, axis=1) == 0, label, len(class_names) - 1)\n",
        "\n",
        "    # Using one-hot in order to use AUC\n",
        "    return (embeddings, tf.one_hot(labels, len(class_names)))\n",
        "\n",
        "\n",
        "def dataframe_to_dataset(dataframe, batch_size=64):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (dataframe[\"filename\"], dataframe[\"label\"])\n",
        "    )\n",
        "\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: filepath_to_embeddings(x, y),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "    ).unbatch()\n",
        "\n",
        "    return dataset.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = dataframe_to_dataset(train_df)\n",
        "valid_ds = dataframe_to_dataset(valid_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wQubygkPTvj"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "The model that we use consists of:\n",
        "\n",
        "* An input layer which is the embedding output of the Yamnet classifier.\n",
        "* 4 dense hidden layers and 4 dropout layers.\n",
        "* An output dense layer.\n",
        "\n",
        "The model's hyperparameters were selected using\n",
        "[KerasTuner](https://keras.io/keras_tuner/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjlwgXx8PTvj",
        "outputId": "cb5c08d9-259b-43a6-a9c9-f081a9933571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"accent_recognition\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (InputLayer)      [(None, 1024)]            0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               262400    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 384)               98688     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 384)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 192)               73920     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 192)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 384)               74112     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 384)               0         \n",
            "                                                                 \n",
            " ouput (Dense)               (None, 7)                 2695      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 511,815\n",
            "Trainable params: 511,815\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "\n",
        "def build_and_compile_model():\n",
        "    inputs = keras.layers.Input(shape=(1024), name=\"embedding\")\n",
        "\n",
        "    x = keras.layers.Dense(256, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "    x = keras.layers.Dropout(0.15, name=\"dropout_1\")(x)\n",
        "\n",
        "    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_2\")(x)\n",
        "    x = keras.layers.Dropout(0.2, name=\"dropout_2\")(x)\n",
        "\n",
        "    x = keras.layers.Dense(192, activation=\"relu\", name=\"dense_3\")(x)\n",
        "    x = keras.layers.Dropout(0.25, name=\"dropout_3\")(x)\n",
        "\n",
        "    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_4\")(x)\n",
        "    x = keras.layers.Dropout(0.2, name=\"dropout_4\")(x)\n",
        "\n",
        "    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\", name=\"ouput\")(\n",
        "        x\n",
        "    )\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"accent_recognition\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1.9644e-5),\n",
        "        loss=keras.losses.CategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_and_compile_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXz0C75LPTvl"
      },
      "source": [
        "## Class weights calculation\n",
        "\n",
        "Since the dataset is quite unbalanced, we wil use `class_weight` argument during training.\n",
        "\n",
        "Getting the class weights is a little tricky because even though we know the number of\n",
        "audio files for each class, it does not represent the number of samples for that class\n",
        "since Yamnet transforms each audio file into multiple audio samples of 0.96 seconds each.\n",
        "So every audio file will be split into a number of samples that is proportional to its length.\n",
        "\n",
        "Therefore, to get those weights, we have to calculate the number of samples for each class\n",
        "after preprocessing through Yamnet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "mOpnFJoCPTvl",
        "outputId": "a79abe96-3963-4971-bf4f-74256ac46b32"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-33330966307c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     class_counts = class_counts + tf.math.bincount(\n\u001b[1;32m      5\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
          ]
        }
      ],
      "source": [
        "class_counts = tf.zeros(shape=(len(class_names),), dtype=tf.int32)\n",
        "\n",
        "for x, y in iter(train_ds):\n",
        "    class_counts = class_counts + tf.math.bincount(\n",
        "        tf.cast(tf.math.argmax(y, axis=1), tf.int32), minlength=len(class_names)\n",
        "    )\n",
        "\n",
        "class_weight = {\n",
        "    i: tf.math.reduce_sum(class_counts).numpy() / class_counts[i].numpy()\n",
        "    for i in range(len(class_counts))\n",
        "}\n",
        "\n",
        "print(class_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP6qgncyPTvl"
      },
      "source": [
        "## Callbacks\n",
        "\n",
        "We use Keras callbacks in order to:\n",
        "\n",
        "* Stop whenever the validation AUC stops improving.\n",
        "* Save the best model.\n",
        "* Call TensorBoard in order to later view the training and validation logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a5nO7rmPTvl"
      },
      "outputs": [],
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_auc\", patience=10, restore_best_weights=True\n",
        ")\n",
        "\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    MODEL_NAME + \".h5\", monitor=\"val_auc\", save_best_only=True\n",
        ")\n",
        "\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(\n",
        "    os.path.join(os.curdir, \"logs\", model.name)\n",
        ")\n",
        "\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdeYD_tnPTvl"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoFBIygWPTvl"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_ds,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacks,\n",
        "    verbose=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFAcsK5iPTvl"
      },
      "source": [
        "## Results\n",
        "\n",
        "Let's plot the training and validation AUC and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etzd5i2fPTvl"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
        "\n",
        "axs[0].plot(range(EPOCHS), history.history[\"accuracy\"], label=\"Training\")\n",
        "axs[0].plot(range(EPOCHS), history.history[\"val_accuracy\"], label=\"Validation\")\n",
        "axs[0].set_xlabel(\"Epochs\")\n",
        "axs[0].set_title(\"Training & Validation Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].plot(range(EPOCHS), history.history[\"auc\"], label=\"Training\")\n",
        "axs[1].plot(range(EPOCHS), history.history[\"val_auc\"], label=\"Validation\")\n",
        "axs[1].set_xlabel(\"Epochs\")\n",
        "axs[1].set_title(\"Training & Validation AUC\")\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4rGaCCCPTvm"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbsPJu_UPTvm"
      },
      "outputs": [],
      "source": [
        "train_loss, train_acc, train_auc = model.evaluate(train_ds)\n",
        "valid_loss, valid_acc, valid_auc = model.evaluate(valid_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMugjywePTvm"
      },
      "source": [
        "Let's try to compare our model's performance to Yamnet's using one of Yamnet metrics (d-prime)\n",
        "Yamnet achieved a d-prime value of 2.318.\n",
        "Let's check our model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-kwEahjPTvm"
      },
      "outputs": [],
      "source": [
        "# The following function calculates the d-prime score from the AUC\n",
        "def d_prime(auc):\n",
        "    standard_normal = stats.norm()\n",
        "    d_prime = standard_normal.ppf(auc) * np.sqrt(2.0)\n",
        "    return d_prime\n",
        "\n",
        "\n",
        "print(\n",
        "    \"train d-prime: {0:.3f}, validation d-prime: {1:.3f}\".format(\n",
        "        d_prime(train_auc), d_prime(valid_auc)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrRylIIHPTvm"
      },
      "source": [
        "We can see that the model achieves the following results:\n",
        "\n",
        "Results    | Training  | Validation\n",
        "-----------|-----------|------------\n",
        "Accuracy   | 54%       | 51%\n",
        "AUC        | 0.91      | 0.89\n",
        "d-prime    | 1.882     | 1.740"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3BPAQotPTvm"
      },
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "Let's now plot the confusion matrix for the validation dataset.\n",
        "\n",
        "The confusion matrix lets us see, for every class, not only how many samples were correctly classified,\n",
        "but also which other classes were the samples confused with.\n",
        "\n",
        "It allows us to calculate the precision and recall for every class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HgSWJLNPTvm"
      },
      "outputs": [],
      "source": [
        "# Create x and y tensors\n",
        "x_valid = None\n",
        "y_valid = None\n",
        "\n",
        "for x, y in iter(valid_ds):\n",
        "    if x_valid is None:\n",
        "        x_valid = x.numpy()\n",
        "        y_valid = y.numpy()\n",
        "    else:\n",
        "        x_valid = np.concatenate((x_valid, x.numpy()), axis=0)\n",
        "        y_valid = np.concatenate((y_valid, y.numpy()), axis=0)\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = model.predict(x_valid)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_mtx = tf.math.confusion_matrix(\n",
        "    np.argmax(y_valid, axis=1), np.argmax(y_pred, axis=1)\n",
        ")\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    confusion_mtx, xticklabels=class_names, yticklabels=class_names, annot=True, fmt=\"g\"\n",
        ")\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Label\")\n",
        "plt.title(\"Validation Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLWSUA5gPTvm"
      },
      "source": [
        "## Precision & recall\n",
        "\n",
        "For every class:\n",
        "\n",
        "* Recall is the ratio of correctly classified samples i.e. it shows how many samples\n",
        "of this specific class, the model is able to detect.\n",
        "It is the ratio of diagonal elements to the sum of all elements in the row.\n",
        "* Precision shows the accuracy of the classifier. It is the ratio of correctly predicted\n",
        "samples among the ones classified as belonging to this class.\n",
        "It is the ratio of diagonal elements to the sum of all elements in the column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AE5JJkKPTvm"
      },
      "outputs": [],
      "source": [
        "for i, label in enumerate(class_names):\n",
        "    precision = confusion_mtx[i, i] / np.sum(confusion_mtx[:, i])\n",
        "    recall = confusion_mtx[i, i] / np.sum(confusion_mtx[i, :])\n",
        "    print(\n",
        "        \"{0:15} Precision:{1:.2f}%; Recall:{2:.2f}%\".format(\n",
        "            label, precision * 100, recall * 100\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnlTHVk-PTvm"
      },
      "source": [
        "## Run inference on test data\n",
        "\n",
        "Let's now run a test on a single audio file.\n",
        "Let's check this example from [The Scottish Voice](https://www.thescottishvoice.org.uk/home/)\n",
        "\n",
        "We will:\n",
        "\n",
        "* Download the mp3 file.\n",
        "* Convert it to a 16k wav file.\n",
        "* Run the model on the wav file.\n",
        "* Plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EEU7jtfPTvm"
      },
      "outputs": [],
      "source": [
        "filename = \"audio-sample-Stuart\"\n",
        "url = \"https://www.thescottishvoice.org.uk/files/cm/files/\"\n",
        "\n",
        "if os.path.exists(filename + \".wav\") == False:\n",
        "    print(f\"Downloading {filename}.mp3 from {url}\")\n",
        "    command = f\"wget {url}{filename}.mp3\"\n",
        "    os.system(command)\n",
        "\n",
        "    print(f\"Converting mp3 to wav and resampling to 16 kHZ\")\n",
        "    command = (\n",
        "        f\"ffmpeg -hide_banner -loglevel panic -y -i {filename}.mp3 -acodec \"\n",
        "        f\"pcm_s16le -ac 1 -ar 16000 {filename}.wav\"\n",
        "    )\n",
        "    os.system(command)\n",
        "\n",
        "filename = filename + \".wav\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxdNCTbUPTvm"
      },
      "source": [
        "The below function `yamnet_class_names_from_csv` was copied and very slightly changed\n",
        "from this [Yamnet Notebook](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/yamnet.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFbW1WbcPTvm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def yamnet_class_names_from_csv(yamnet_class_map_csv_text):\n",
        "    \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
        "    yamnet_class_map_csv = io.StringIO(yamnet_class_map_csv_text)\n",
        "    yamnet_class_names = [\n",
        "        name for (class_index, mid, name) in csv.reader(yamnet_class_map_csv)\n",
        "    ]\n",
        "    yamnet_class_names = yamnet_class_names[1:]  # Skip CSV header\n",
        "    return yamnet_class_names\n",
        "\n",
        "\n",
        "yamnet_class_map_path = yamnet_model.class_map_path().numpy()\n",
        "yamnet_class_names = yamnet_class_names_from_csv(\n",
        "    tf.io.read_file(yamnet_class_map_path).numpy().decode(\"utf-8\")\n",
        ")\n",
        "\n",
        "\n",
        "def calculate_number_of_non_speech(scores):\n",
        "    number_of_non_speech = tf.math.reduce_sum(\n",
        "        tf.where(tf.math.argmax(scores, axis=1, output_type=tf.int32) != 0, 1, 0)\n",
        "    )\n",
        "\n",
        "    return number_of_non_speech\n",
        "\n",
        "\n",
        "def filename_to_predictions(filename):\n",
        "    # Load 16k audio wave\n",
        "    audio_wav = load_16k_audio_wav(filename)\n",
        "\n",
        "    # Get audio embeddings & scores.\n",
        "    scores, embeddings, mel_spectrogram = yamnet_model(audio_wav)\n",
        "\n",
        "    print(\n",
        "        \"Out of {} samples, {} are not speech\".format(\n",
        "            scores.shape[0], calculate_number_of_non_speech(scores)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Predict the output of the accent recognition model with embeddings as input\n",
        "    predictions = model.predict(embeddings)\n",
        "\n",
        "    return audio_wav, predictions, mel_spectrogram\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA4rL6wbPTvm"
      },
      "source": [
        "Let's run the model on the audio file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgY08Jh4PTvm"
      },
      "outputs": [],
      "source": [
        "audio_wav, predictions, mel_spectrogram = filename_to_predictions(filename)\n",
        "\n",
        "infered_class = class_names[predictions.mean(axis=0).argmax()]\n",
        "print(f\"The main accent is: {infered_class} English\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41Sb5cE-PTvm"
      },
      "source": [
        "Listen to the audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AL2JRk3PTvn"
      },
      "outputs": [],
      "source": [
        "Audio(audio_wav, rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c5NomdSPTvn"
      },
      "source": [
        "The below function was copied from this [Yamnet notebook](tinyurl.com/4a8xn7at) and adjusted to our need.\n",
        "\n",
        "This function plots the following:\n",
        "\n",
        "* Audio waveform\n",
        "* Mel spectrogram\n",
        "* Predictions for every time step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePZzq3d4PTvn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the waveform.\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(audio_wav)\n",
        "plt.xlim([0, len(audio_wav)])\n",
        "\n",
        "# Plot the log-mel spectrogram (returned by the model).\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.imshow(\n",
        "    mel_spectrogram.numpy().T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\"\n",
        ")\n",
        "\n",
        "# Plot and label the model output scores for the top-scoring classes.\n",
        "mean_predictions = np.mean(predictions, axis=0)\n",
        "\n",
        "top_class_indices = np.argsort(mean_predictions)[::-1]\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.imshow(\n",
        "    predictions[:, top_class_indices].T,\n",
        "    aspect=\"auto\",\n",
        "    interpolation=\"nearest\",\n",
        "    cmap=\"gray_r\",\n",
        ")\n",
        "\n",
        "# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n",
        "# values from the model documentation\n",
        "patch_padding = (0.025 / 2) / 0.01\n",
        "plt.xlim([-patch_padding - 0.5, predictions.shape[0] + patch_padding - 0.5])\n",
        "# Label the top_N classes.\n",
        "yticks = range(0, len(class_names), 1)\n",
        "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
        "_ = plt.ylim(-0.5 + np.array([len(class_names), 0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "uk_ireland_accent_recognition",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}