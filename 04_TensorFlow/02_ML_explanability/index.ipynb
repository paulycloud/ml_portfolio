{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Flight Predictions with Explainable AI with TensorFlow on Vertex AI \n",
    "\n",
    "[Qwiklabs](https://www.cloudskillsboost.google/focuses/3384?catalog_rank=%7B%22rank%22%3A7%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=17387787)\n",
    "\n",
    "\n",
    "Key Concepts: \n",
    "- Explanable AI\n",
    "- Vertex AI \n",
    "- Vertex AI Workbench\n",
    "- TensorFlow\n",
    "\n",
    "This lab involves creating a Vertex AI Workbench instance on which you develop a TensorFlow model in Jupyter notebook. You train the model, create an input data pipeline, deploy it to an endpoint, and get predictions.\n",
    "\n",
    "You will explore the machine learning approach by using the TensorFlow library to carry out GPU-accelerated training. \n",
    "\n",
    "You will train a logistic regression model on all of the input values and learned that the model was unable to effectively use the new features like airport locations.\n",
    "\n",
    "\n",
    "\n",
    "# Objective \n",
    "In this lab you create a Vertex AI Workbench instance on which you develop a TensorFlow model in Jupyter notebook to create Flight predictions. You train the model, create an input data pipeline, deploy it to an endpoint, and get predictions.\n",
    "\n",
    "- Deploy Vertex AI Workbench Instances \n",
    "- Create minimal training, validation data \n",
    "- Create the input data pipeline \n",
    "- Deploy model to Vertex AI\n",
    "- Deploy Explainable AI model to Vertex AI  \n",
    "\n",
    "## Tasks\n",
    "1. Deploy Vertex AI Workbench instance\n",
    "1. Create minimal training and validation data\n",
    "1. Create the input data \n",
    "1. Create, train & evaluate TensorFlow Model \n",
    "1. Deploy flights model to Vertex AI\n",
    "1. Model Explainability \n",
    "1. Invoke the deployed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Deploy Vertex AI Workbench instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create minimal training and validation data\n",
    "\n",
    "Exporting the files that contain training, validation data: \n",
    " \n",
    "1. Create training dataset flights_train_data for model training:\n",
    "1. Create the evaluation dataset flights_eval_data for model evaluation:\n",
    "1. Create the full dataset flights_all_data using the following code.\n",
    "1. Export the training, validation, and full datasets to CSV file format the Google Cloud Storage bucket:\n",
    "1. List exported objects to Google Cloud Storage bucket using the following code.\n",
    "\n",
    "\n",
    "Import python libraries and set environment variables:\n",
    "```python\n",
    "import os, json, math, shutil \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "# envrionment variables used by bash cells \n",
    "PROJECT=!(gcloud config get-value project)\n",
    "PROJECT=PROJECT[0]\n",
    "REGION = 'us-central1'\n",
    "BUCKET='{}-dsongcp'.format(PROJECT)\n",
    "os.environ['ENDPOINT_NAME'] = 'flights'\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TF_VERSION']='2-' + tf.__version__[2:3]\n",
    "\n",
    "```\n",
    "\n",
    "## Export files that contain training, validation data: \n",
    "When the lab spins up, a few tables are created in the BigQuery dataset. In this section, you use BigQuery to create temporary tables in BigQuery that contain the data we need, and then export the table to CSV files on Google Cloud Storage. You then delete the temporary table. Moving further, read and process those CSV data files to create the training, validation, and full datasets you need for a custom TensorFlow model.\n",
    "\n",
    "\n",
    "### 1. Create training dataset ```flights_train_data``` for model training:\n",
    "\n",
    "```sql\n",
    "%%bigquery \n",
    "\n",
    "CREATE OR REPLACE TABLE dsogncp.flights_train_data AS \n",
    "SELECT \n",
    "    IF(arr_delay < 15, 1.0, 0.0) AS ontime, \n",
    "    dep_delay, \n",
    "    taxi_out, \n",
    "    distance, \n",
    "    origin, \n",
    "    dest, \n",
    "    EXTRACT (hour from dep_time) AS dep_hour, \n",
    "    IF (EXTRACT (dayofweek FROM dep_time) BETWEEN 2 AND 6,1,0 ) AS \n",
    "is_weekday, \n",
    "    UNIQUE_CARRIER AS carrier, \n",
    "    dep_airport_lat, \n",
    "    dep_airport_lon, \n",
    "    arr_airport_lat, \n",
    "    arr_airport_lon, \n",
    "FROM dsongcp.flights_tzcorr f \n",
    "JOIN dsongcp.trainday t \n",
    "ON f.FL_DATE = t.FL_DATA \n",
    "WHERE \n",
    "    f.CANCELLED = False AND \n",
    "    f.DIVERTED = False AND \n",
    "    is_train_day = 'True'\n",
    "```\n",
    "\n",
    "### 2. Create the evaluation dataset ```flights_eval_data``` for model evaluation:\n",
    "```sql\n",
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE dsongcp.flights_eval_data AS\n",
    "SELECT\n",
    "  IF(arr_delay < 15, 1.0, 0.0) AS ontime,\n",
    "  dep_delay,\n",
    "  taxi_out,\n",
    "  distance,\n",
    "  origin,\n",
    "  dest,\n",
    "  EXTRACT(hour FROM dep_time) AS dep_hour,\n",
    "  IF (EXTRACT(dayofweek FROM dep_time) BETWEEN 2 AND 6, 1, 0) AS is_weekday,\n",
    "  UNIQUE_CARRIER AS carrier,\n",
    "  dep_airport_lat,\n",
    "  dep_airport_lon,\n",
    "  arr_airport_lat,\n",
    "  arr_airport_lon\n",
    "FROM dsongcp.flights_tzcorr f\n",
    "JOIN dsongcp.trainday t\n",
    "ON f.FL_DATE = t.FL_DATE\n",
    "WHERE\n",
    "  f.CANCELLED = False AND \n",
    "  f.DIVERTED = False AND\n",
    "  is_train_day = 'False'\n",
    "\n",
    "```\n",
    "\n",
    "### 3. Create the full dataset flights_all_data using the following code.\n",
    "\n",
    "```sql\n",
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE dsongcp.flights_all_data AS\n",
    "SELECT\n",
    "  IF(arr_delay < 15, 1.0, 0.0) AS ontime,\n",
    "  dep_delay,\n",
    "  taxi_out,\n",
    "  distance,\n",
    "  origin,\n",
    "  dest,\n",
    "  EXTRACT(hour FROM dep_time) AS dep_hour,\n",
    "  IF (EXTRACT(dayofweek FROM dep_time) BETWEEN 2 AND 6, 1, 0) AS is_weekday,\n",
    "  UNIQUE_CARRIER AS carrier,\n",
    "  dep_airport_lat,\n",
    "  dep_airport_lon,\n",
    "  arr_airport_lat,\n",
    "  arr_airport_lon,\n",
    "  IF (is_train_day = 'True',\n",
    "      IF(ABS(MOD(FARM_FINGERPRINT(CAST(f.FL_DATE AS STRING)), 100)) < 60, 'TRAIN', 'VALIDATE'),\n",
    "      'TEST') AS data_split\n",
    "FROM dsongcp.flights_tzcorr f\n",
    "JOIN dsongcp.trainday t\n",
    "ON f.FL_DATE = t.FL_DATE\n",
    "WHERE\n",
    "  f.CANCELLED = False AND \n",
    "  f.DIVERTED = False\n",
    "```\n",
    "\n",
    "### 4. Export the training, validation, and full datasets to CSV file format the Google Cloud Storage bucket:\n",
    "```bash\n",
    "%%bash\n",
    "PROJECT=$(gcloud config get-value project)\n",
    "for dataset in \"train\" \"eval\" \"all\"; do\n",
    "  TABLE=dsongcp.flights_${dataset}_data\n",
    "  CSV=gs://${BUCKET}/ch9/data/${dataset}.csv\n",
    "  echo \"Exporting ${TABLE} to ${CSV} and deleting table\"\n",
    "  bq --project_id=${PROJECT} extract --destination_format=CSV $TABLE $CSV\n",
    "  bq --project_id=${PROJECT} rm -f $TABLE\n",
    "done\n",
    "\n",
    "```\n",
    "\n",
    "### 5. List exported objects to Google Cloud Storage bucket using the following code.\n",
    "```bash\n",
    "!gsutil ls -lh gs://{BUCKET}/ch9/data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create the input data \n",
    "\n",
    "Activities in this section: \n",
    "1. Set up the notebook \n",
    "1. set the epoch training cycles \n",
    "1. assign the training and validation data uris \n",
    "1. Set up model parameters\n",
    "1. Read data into Tensorflow\n",
    "\n",
    "### Set up the notebook\n",
    "For development purposes, train for a few epochs. That's why the NUM_EXAMPLES is so low.\n",
    "\n",
    "### set the epoch training cycles \n",
    "```python\n",
    "DEVELOP_MODE = True\n",
    "NUM_EXAMPLES = 5000*1000\n",
    "```\n",
    "\n",
    "### assign the training and validation data uris \n",
    "Assign your training and validation data URI to training_data_uri and validation_data_uri respectively.\n",
    "\n",
    "```python\n",
    "training_data_uri = 'gs://{}/ch9/data/train*'.format(BUCKET)\n",
    "validation_data_uri = 'gs://{}/ch9/data/eval*'.format(BUCKET)\n",
    "```\n",
    "\n",
    "### Set up model parameters\n",
    "```python\n",
    "NBUCKETS = 5\n",
    "NEMBEDS = 3\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "DNN_HIDDEN_UNITS = '64,32'\n",
    "\n",
    "```\n",
    "\n",
    "### Read data into Tensorflow\n",
    "#### 1. Read the CSV \n",
    "```python\n",
    "if DEVELOP_MODE: \n",
    "    train_df = tf.data.experimental.make_csv_dataset(training_data_uri, batch_size=5)\n",
    "    for n, data in enumerate (train_df): \n",
    "        numpy_data = {k: v.numpy() for k, v in data.items()}\n",
    "        print(n, numpy_data)\n",
    "        if n==1: break\n",
    "```\n",
    "\n",
    "Write ```features_and_labels()``` and ```read_dataset()``` functions. The ```read_dataset() ``` function reads the training data, yielding batch_size examples each time, and allows you to stop iterating once a certain number of examples have been read.\n",
    "\n",
    "The dataset contains all the columns in the CSV file, named according to the header line. The data consists of both features and the label. It’s better to separate them by writing the ```features_and_labels()``` function to make the later code easier to read. Hence, we’ll apply a pop() function to the dictionary and return a tuple of features and labels).\n",
    "\n",
    "#### Enter and run the following code: \n",
    "\n",
    "```python\n",
    "def features_and_labels(features): \n",
    "    label = features.pop('ontime')\n",
    "    return features, label\n",
    "\n",
    "def read_dataset(pattern, batch_size, model=tf.estimator.ModeKeys.TRAIN, truncate=None):\n",
    "    dataset = tf.data.experiemntal.make_csv_dataset(pattern, batch_size, num_epochs=1)\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN: \n",
    "       dataset = dataset.shuffle(batch_size*10)\n",
    "       dataset = dataset.repeat()\n",
    "       dataset = dataset.prefetch(1)\n",
    "    if truncate is not None: \n",
    "        dataset = dataset.take(truncate)\n",
    "    return dataset\n",
    "\n",
    "if DEVELOP_MODE:\n",
    "    print(\"Checking input pipeline\")\n",
    "    one_item = read_dataset(training_data_uri, batch_size=2, truncate=1)\n",
    "    print(list(one_item)) # should print one batch of 2 items\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create, train & evaluate TensorFlow Model \n",
    "Typically you create one feature for every column in our tabular data. Keras supports feature columns, opening up the ability to represent structured data using standard feature engineering techniques like embedding, bucketizing, and feature crosses. As numeric data can be passed in directly to the ML model, keep the real-valued columns separate from the sparse (or string) columns:\n",
    "\n",
    "This section covers the following: \n",
    "1. Create feature columns with keras\n",
    "1. Map the input layers \n",
    "1. Break up the country values into grid points by Bucketing \n",
    "1. Train and evaluate the model\n",
    "1. Manage the check point. (Save, & delete already existing checkpoints)\n",
    "1. Build a wide-and-deep model\n",
    "1. Use the ```train_dataset``` for model training and ```eval_dataset``` for model evaluation \n",
    "1. Visualize the model loss and model accuracy using ```matplotlib.pyplot```\n",
    "1. Export the trained model\n",
    "\n",
    "### Create feature columns with keras\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "real = {\n",
    "    colname : tf.feature_column.numeric_column(colname) \n",
    "          for colname in \n",
    "            (\n",
    "                'dep_delay,taxi_out,distance,dep_hour,is_weekday,' +\n",
    "                'dep_airport_lat,dep_airport_lon,' +\n",
    "                'arr_airport_lat,arr_airport_lon'\n",
    "            ).split(',')\n",
    "}\n",
    "sparse = {\n",
    "      'carrier': tf.feature_column.categorical_column_with_vocabulary_list('carrier',\n",
    "                  vocabulary_list='AS,VX,F9,UA,US,WN,HA,EV,MQ,DL,OO,B6,NK,AA'.split(',')),\n",
    "      'origin' : tf.feature_column.categorical_column_with_hash_bucket('origin', hash_bucket_size=1000),\n",
    "      'dest'   : tf.feature_column.categorical_column_with_hash_bucket('dest', hash_bucket_size=1000),\n",
    "}\n",
    "```\n",
    "\n",
    "All these features come directly from the input file (and are provided by any client that wants a prediction for a flight). \n",
    "Input layers map 1:1 to the input features and their types, so rather than repeat the column names, you create an input layer for each of these columns, specifying the right data type (either a float or a string).\n",
    "\n",
    "\n",
    "### Map the input layers \n",
    "```python\n",
    "\n",
    "inputs = {\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in real.keys()\n",
    "}\n",
    "\n",
    "inputs.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='string')\n",
    "    for colname in sparse.keys()\n",
    "})\n",
    "\n",
    "```\n",
    "\n",
    "### Break up the country values into grid points by Bucketing \n",
    "Real-valued columns whose precision is overkill (thus, likely to cause overfitting) can be discretized and made into categorical columns. For example, if you have a column for the age of the aircraft, you might discretize into just three bins—less than 5 years old, 5 to 20 years old, and more than 20 years old. Use the discretization shortcut: you can discretize the latitudes and longitudes and cross the buckets—this results in breaking up the country into grids and yield the grid point into which a specific latitude and longitude falls.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# set latitude and longitude \n",
    "latbuckets = np.linspace(20.0, 50.0, NBUCKETS).tolist()  # USA\n",
    "lonbuckets = np.linspace(-120.0, -70.0, NBUCKETS).tolist() # USA\n",
    "\n",
    "disc = {}\n",
    "\n",
    "disc.update({\n",
    "       'd_{}'.format(key) : tf.feature_column.bucketized_column(real[key], latbuckets) \n",
    "          for key in ['dep_airport_lat', 'arr_airport_lat']\n",
    "})\n",
    "disc.update({\n",
    "       'd_{}'.format(key) : tf.feature_column.bucketized_column(real[key], lonbuckets) \n",
    "          for key in ['dep_airport_lon', 'arr_airport_lon']\n",
    "})\n",
    "\n",
    "# cross columns that make sense in combination\n",
    "sparse['dep_loc'] = tf.feature_column.crossed_column(\n",
    "    [disc['d_dep_airport_lat'], disc['d_dep_airport_lon']], NBUCKETS*NBUCKETS)\n",
    "\n",
    "sparse['arr_loc'] = tf.feature_column.crossed_column(\n",
    "    [disc['d_arr_airport_lat'], disc['d_arr_airport_lon']], NBUCKETS*NBUCKETS)\n",
    "\n",
    "sparse['dep_arr'] = tf.feature_column.crossed_column([sparse['dep_loc'], sparse['arr_loc']], NBUCKETS ** 4)\n",
    "\n",
    "# embed all the sparse columns\n",
    "\n",
    "embed = {\n",
    "       'embed_{}'.format(colname) : tf.feature_column.embedding_column(col, NEMBEDS)\n",
    "          for colname, col in sparse.items()\n",
    "}\n",
    "real.update(embed)\n",
    "\n",
    "# one-hot encode the sparse columns\n",
    "\n",
    "sparse = {\n",
    "    colname : tf.feature_column.indicator_column(col)\n",
    "          for colname, col in sparse.items()\n",
    "}\n",
    "\n",
    "if DEVELOP_MODE:\n",
    "    print(sparse.keys())\n",
    "    print(real.keys())\n",
    "\n",
    "```\n",
    "\n",
    "### Train and evaluate the model. \n",
    "\n",
    "Save the checkpoint \n",
    "\n",
    "```python\n",
    "\n",
    "output_dir = 'gs://{}/ch9/trained_model'.format(BUCKET)\n",
    "os.envrion['OUTDIR'] = output_dir #needed for deployment \n",
    "print('Writing trained model to {}'.format(output_dir))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Manage the check point. (Save, & delete already existing checkpoints)\n",
    "\n",
    "```bash\n",
    "!gsutil -m rm -rf $OUTDIR\n",
    "```\n",
    "\n",
    "This reports an error stating ```CommandException: 1 files/objects could not be removed``` because the model has not yet been saved. \n",
    "The error indicates that there are no files present in the target location. You must be certain that this location is empty before attempting to save the model and this command guarantees that.\n",
    "\n",
    "### Build a wide-and-deep model\n",
    "\n",
    "With the sparse and real feature columns thus enhanced beyond the raw inputs, you can create a wide_and_deep_classifier passing in the linear and deep feature columns separately:\n",
    "\n",
    "```python\n",
    "\n",
    "# Build a wide-and-deep model \n",
    "\n",
    "def wide_and_deep_classifier(inputs, linear_feature_columns, dnn_feature_columns, dnn_hidden_units):\n",
    "    deep = tf.keras.layers.DenseFeatures(dnn_feature_columns, name='deep_inputs')(inputs)\n",
    "    layers = [int(x) for x in dnn_hidden_units.split(',') ]\n",
    "        for layerno, numnodes in enumerate(layers):\n",
    "            deep   = tf.keras.layers.Dense(numnodes, activation='relu', name='dnn_{}'.format(layerno+1))(deep)\n",
    "            wide   = tf.keras.layers.DenseFeatures(linear_feature_columns, name='wide_inputs')(inputs)\n",
    "            both   = tf.keras.layers.concatenate([deep,wide], name='both')\n",
    "            output = tf.keras.layers.Dense(1, activation='sigmoid', name='pred')(both)\n",
    "            model  = tf.keras.Model(inputs, output)\n",
    "            model.compile(optimizer='adam', \n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = wide_and_deep_classifier (\n",
    "        inputs, \n",
    "        linear_feature_columns = sparse.values(), \n",
    "        dnn_feature_columns = real.values(), \n",
    "        dnn_hidden_units = DNN_HIDDEN_UNITS\n",
    ")\n",
    "\n",
    "tf.keras.util.plot_model(model, 'flight_model.png', show_shapes=False, rankir='LR')\n",
    "\n",
    "```\n",
    "\n",
    "![alt text](./resources/02_01.png \"Wide and Deep \")\n",
    "\n",
    "\n",
    "### Use the ```train_dataset``` for model training and ```eval_dataset``` for model evaluation \n",
    "\n",
    "```python\n",
    "\n",
    "# training and evaluation dataset\n",
    "train_batch_size = TRAIN_BATCH_SIZE\n",
    "if DEVELOP_MODE:\n",
    "    eval_batch_size = 100\n",
    "    steps_per_epoch = 3\n",
    "    epochs = 2\n",
    "    num_eval_examples = eval_batch_size*10\n",
    "else:\n",
    "    eval_batch_size = 100\n",
    "    steps_per_epoch = NUM_EXAMPLES // train_batch_size\n",
    "    epochs = 10\n",
    "    num_eval_examples = eval_batch_size * 100\n",
    "train_dataset = read_dataset(training_data_uri, train_batch_size)\n",
    "eval_dataset = read_dataset(validation_data_uri, eval_batch_size, tf.estimator.ModeKeys.EVAL, num_eval_examples)\n",
    "checkpoint_path = '{}/checkpoints/flights.cpt'.format(output_dir)\n",
    "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=eval_dataset,\n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "```\n",
    "\n",
    "### Visualize the model loss and model accuracy using ```matplotlib.pyplot```\n",
    "\n",
    "```python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "nrows = 1\n",
    "ncols = 2\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "for idx, key in enumerate(['loss', 'accuracy']):\n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "    plt.plot(history.history[key])\n",
    "    plt.plot(history.history['val_{}'.format(key)])\n",
    "    plt.title('model {}'.format(key))\n",
    "    plt.ylabel(key)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left');\n",
    "\n",
    "```\n",
    "![alt text](./resources/02_02.png \"Visualize \")\n",
    "\n",
    "### Export the trained model\n",
    "\n",
    "Save the model artifacts to the google Cloud storage bucket \n",
    "\n",
    "```python\n",
    "\n",
    "import time \n",
    "export_dir = '{}/export/flights_{}'.format(output_dir, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "print('Exporting to {}'.format(export_dir))\n",
    "tf.saved_model.save(model,export_dir)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Deploy flights model to Vertex AI\n",
    "Vertex AI provides a fully managed, autoscaling, serverless environment for Machine Learning models. You get the benefits of paying for any compute resources (such as CPUs or GPUs) only when you are using them. Because the models are containerized, dependency management is taken care of. The Endpoints take care of traffic splits, allowing you to do A/B testing in a convenient way.\n",
    "\n",
    "This section covers: \n",
    "1. Deploy the model endpoint \"flights\" to vertex AI\n",
    "1. Create a test input file \n",
    "1. Make prediction from the model endpoint\n",
    "1. Invoke the model \n",
    "\n",
    "\n",
    "### Deploy the model endpoint \"flights\" to vertex AI\n",
    "Create hte model endpoint ```flights``` using the following code cell and delete any existing models with the same name \n",
    "\n",
    "```bash\n",
    "%%bash\n",
    "# note TF_VERSION and ENDPOINT_NAME set in 1st cell\n",
    "# TF_VERSION=2-6\n",
    "# ENDPOINT_NAME=flights\n",
    "\n",
    "TIMESTAMP=$(date +%Y%m%d-%H%M%S)\n",
    "MODEL_NAME=${ENDPOINT_NAME}-${TIMESTAMP}\n",
    "EXPORT_PATH=$(gsutil ls ${OUTDIR}/export | tail -1)\n",
    "echo $EXPORT_PATH\n",
    "\n",
    "# create the model endpoint for deploying the model\n",
    "if [[ $(gcloud beta ai endpoints list --region=$REGION \\\n",
    "        --format='value(DISPLAY_NAME)' --filter=display_name=${ENDPOINT_NAME}) ]]; then\n",
    "    echo \"Endpoint for $MODEL_NAME already exists\"\n",
    "else\n",
    "    echo \"Creating Endpoint for $MODEL_NAME\"\n",
    "    gcloud beta ai endpoints create --region=${REGION} --display-name=${ENDPOINT_NAME}\n",
    "if\n",
    "ENDPOINT_ID=$(gcloud beta ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)' --filter=display_name=${ENDPOINT_NAME})\n",
    "echo \"ENDPOINT_ID=$ENDPOINT_ID\"\n",
    "\n",
    "# delete any existing models with this name\n",
    "for MODEL_ID in $(gcloud beta ai models list --region=$REGION --format='value(MODEL_ID)' --filter=display_name=${MODEL_NAME}); do\n",
    "    echo \"Deleting existing $MODEL_NAME ... $MODEL_ID \"\n",
    "    gcloud ai models delete --region=$REGION $MODEL_ID\n",
    "done\n",
    "\n",
    "\n",
    "# create the model using the parameters docker conatiner image and artifact uri\n",
    "\n",
    "gcloud beta ai models upload --region=$REGION --display-name=$MODEL_NAME \\\n",
    "     --container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.${TF_VERSION}:latest \\\n",
    "     --artifact-uri=$EXPORT_PATH\n",
    "MODEL_ID=$(gcloud beta ai models list --region=$REGION --format='value(MODEL_ID)' --filter=display_name=${MODEL_NAME})\n",
    "echo \"MODEL_ID=$MODEL_ID\"\n",
    "\n",
    "# deploy the model to the endpoint\n",
    "gcloud beta ai endpoints deploy-model $ENDPOINT_ID \\\n",
    "  --region=$REGION \\\n",
    "  --model=$MODEL_ID \\\n",
    "  --display-name=$MODEL_NAME \\\n",
    "  --machine-type=n1-standard-2 \\\n",
    "  --min-replica-count=1 \\\n",
    "  --max-replica-count=1 \\\n",
    "  --traffic-split=0=100\n",
    "```\n",
    "\n",
    "### Create a test input file \n",
    "Create a test input file ```example_input.json``` using the following code: \n",
    "\n",
    "```bash\n",
    "%%writefile example_input.json\n",
    "\n",
    "{\"instances\": [\n",
    "  {\"dep_hour\": 2, \"is_weekday\": 1, \"dep_delay\": 40, \"taxi_out\": 17, \"distance\": 41, \"carrier\": \"AS\", \"dep_airport_lat\": 58.42527778, \"dep_airport_lon\": -135.7075, \"arr_airport_lat\": 58.35472222, \"arr_airport_lon\": -134.57472222, \"origin\": \"GST\", \"dest\": \"JNU\"},\n",
    "  {\"dep_hour\": 22, \"is_weekday\": 0, \"dep_delay\": -7, \"taxi_out\": 7, \"distance\": 201, \"carrier\": \"HA\", \"dep_airport_lat\": 21.97611111, \"dep_airport_lon\": -159.33888889, \"arr_airport_lat\": 20.89861111, \"arr_airport_lon\": -156.43055556, \"origin\": \"LIH\", \"dest\": \"OGG\"}\n",
    "]}\n",
    "\n",
    "```\n",
    "\n",
    "### Make prediction from the model endpoint\n",
    "\n",
    "Make a prediction from the model endpoint. Here you have input data in a JSON file called example_input.json.\n",
    "\n",
    "```bash\n",
    "\n",
    "%%bash\n",
    "ENDPOINT_ID=$(gcloud beta ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)' --filter=display_name=${ENDPOINT_NAME})\n",
    "echo $ENDPOINT_ID\n",
    "gcloud beta ai endpoints predict $ENDPOINT_ID --region=$REGION --json-request=example_input.json\n",
    "\n",
    "```\n",
    "\n",
    "### Invoke the model \n",
    "Here’s how client programs can invoke the model that you deployed.\n",
    "\n",
    "Assume that they have the input data in a JSON file called example_input.json. Now, send an HTTP POST request and you will get the result back as JSON.\n",
    "\n",
    "```bash\n",
    "%%bash\n",
    "PROJECT=$(gcloud config get-value project)\n",
    "ENDPOINT_ID=$(gcloud beta ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)' --filter=display_name=${ENDPOINT_NAME})\n",
    "curl -X POST \\\n",
    "  -H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @example_input.json \\\n",
    "  \"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/endpoints/${ENDPOINT_ID}:predict\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Model Explainability \n",
    "Model explainability is one of the most important problems in machine learning.\n",
    "\n",
    "It's a broad concept of analyzing and understanding the results provided by machine learning models. \n",
    "\n",
    "Explainability in machine learning means you can explain what happens in your model from input to output. It makes models transparent and solves the black box problem. Explainable AI (XAI) is the more formal way to describe this.\n",
    "\n",
    "This section covers: \n",
    "1. Prepare the model\n",
    "1. Prepare the mdoel metadata file \n",
    "1. View the metadata file \n",
    "1. Create and deploy the Explanable AI Model \n",
    "\n",
    "### Prepare the model\n",
    "\n",
    "```bash\n",
    "# save model_dir variable\n",
    "model_dir = $(gsutil ls ${OUTDIR}/export | tail -1)\n",
    "# confirm directory \n",
    "echo $model_dir \n",
    "# call directory iwht cli\n",
    "saved_model_cli show --tag_set serve --signature_def serving_default --dir $model_dir \n",
    "```\n",
    "\n",
    "### Prepare the model metadata file \n",
    "\n",
    "Create a JSON file explanation-metadata.json that contains the metadata describing the Model's input and output for explanation. Here, you use sampled-shapley method used for explanation:\n",
    "\n",
    "```python\n",
    "\n",
    "# data fields\n",
    "cols = ('dep_delay,taxi_out,distance,dep_hour,is_weekday,' +\n",
    "        'dep_airport_lat,dep_airport_lon,' +\n",
    "        'arr_airport_lat,arr_airport_lon,' +\n",
    "        'carrier,origin,dest')\n",
    "\n",
    "# save them as inputs\n",
    "inputs = {x: {\"inputTensorName\": \"{}\".format(x)} \n",
    "        for x in cols.split(',')}\n",
    "\n",
    "# cerate explanation function \n",
    "expl = {\n",
    "    \"inputs\": inputs,\n",
    "    \"outputs\": {\n",
    "    \"pred\": {\n",
    "      \"outputTensorName\": \"pred\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# call explanation function \n",
    "print(expl)\n",
    "with open('explanation-metadata.json', 'w') as ofp:\n",
    "    json.dump(expl, ofp, indent=2)\n",
    "```\n",
    "\n",
    "### View the metadata file \n",
    "\n",
    "```bash\n",
    "!cat explanation-metadata.json\n",
    "```\n",
    "\n",
    "![alt text](./resources/02_03.png \"Metadata inputs display \")\n",
    "\n",
    "\n",
    "### Create and deploy the Explanable AI Model\n",
    "Create the model endpoint flights_xai, upload the model, and deploy it at the model endpoint using the following code:\n",
    "\n",
    "```bash\n",
    "%%bash\n",
    "# note TF_VERSION set in 1st cell, but ENDPOINT_NAME is being changed\n",
    "# TF_VERSION=2-6\n",
    "ENDPOINT_NAME=flights_xai\n",
    "TIMESTAMP=$(date +%Y%m%d-%H%M%S)\n",
    "MODEL_NAME=${ENDPOINT_NAME}-${TIMESTAMP}\n",
    "EXPORT_PATH=$(gsutil ls ${OUTDIR}/export | tail -1)\n",
    "echo $EXPORT_PATH\n",
    "\n",
    "# create the model endpoint for deploying the model\n",
    "if [[ $(gcloud beta ai endpoints list --region=$REGION \\\n",
    "        --format='value(DISPLAY_NAME)' --filter=display_name=${ENDPOINT_NAME}) ]]; then\n",
    "    echo \"Endpoint for $MODEL_NAME already exists\"\n",
    "else\n",
    "    # create model endpoint\n",
    "    echo \"Creating Endpoint for $MODEL_NAME\"\n",
    "    gcloud beta ai endpoints create --region=${REGION} --display-name=${ENDPOINT_NAME}\n",
    "fi\n",
    "ENDPOINT_ID=$(gcloud beta ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)' --filter=display_name=${ENDPOINT_NAME})\n",
    "echo \"ENDPOINT_ID=$ENDPOINT_ID\"\n",
    "\n",
    "# delete any existing models with this name\n",
    "for MODEL_ID in $(gcloud beta ai models list --region=$REGION --format='value(MODEL_ID)' --filter=display_name=${MODEL_NAME}); do\n",
    "    echo \"Deleting existing $MODEL_NAME ... $MODEL_ID \"\n",
    "    gcloud ai models delete --region=$REGION $MODEL_ID\n",
    "done\n",
    "\n",
    "# upload the model using the parameters docker conatiner image, artifact URI, explanation method, \n",
    "# explanation path count and explanation metadata JSON file `explanation-metadata.json`. \n",
    "# Here, you keep number of feature permutations to `10` when approximating the Shapley values for explanation.\n",
    "\n",
    "gcloud beta ai models upload --region=$REGION --display-name=$MODEL_NAME \\\n",
    "     --container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.${TF_VERSION}:latest \\\n",
    "     --artifact-uri=$EXPORT_PATH \\\n",
    "     --explanation-method=sampled-shapley --explanation-path-count=10 --explanation-metadata-file=explanation-metadata.json\n",
    "MODEL_ID=$(gcloud beta ai models list --region=$REGION --format='value(MODEL_ID)' --filter=display_name=${MODEL_NAME})\n",
    "echo \"MODEL_ID=$MODEL_ID\"\n",
    "# deploy the model to the endpoint\n",
    "gcloud beta ai endpoints deploy-model $ENDPOINT_ID \\\n",
    "  --region=$REGION \\\n",
    "  --model=$MODEL_ID \\\n",
    "  --display-name=$MODEL_NAME \\\n",
    "  --machine-type=n1-standard-2 \\\n",
    "  --min-replica-count=1 \\\n",
    "  --max-replica-count=1 \\\n",
    "  --traffic-split=0=100\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Invoke the deployed Model\n",
    "\n",
    "Here’s how client programs can invoke the model you deployed. Assume that they have the input data in a JSON file called example_input.json. Now, send an HTTP POST request and you will get the result back as JSON.\n",
    "\n",
    "```bash\n",
    "%%bash\n",
    "\n",
    "PROJECT=$(gcloud config get-value project)\n",
    "ENDPOINT_NAME=flights_xai\n",
    "ENDPOINT_ID=$(gcloud beta ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)' --filter=display_name=${ENDPOINT_NAME})\n",
    "\n",
    "curl -X POST \\\n",
    "  -H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @example_input.json \\\n",
    "  \"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/${REGION}/endpoints/${ENDPOINT_ID}:explain\"\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
