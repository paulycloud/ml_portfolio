{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "\n",
        "## Using a K-Mean Model to cluster London bicycle hires dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "### Summary\n",
        "\n",
        "This project uses a k-means model in BigQuery ML to identify clusters of data in the London Bicycle Hires public dataset.\n",
        "\n",
        "\n",
        "### Key Concepts: \n",
        "- K-Means \n",
        "- Unsupervised models\n",
        "- Geospatial analysis \n",
        "- Davies-Bouldin Index\n",
        "\n",
        "## Objective \n",
        "\n",
        "- Create a binary K-means clustering model\n",
        "- Make data-driven decisions based on BQML Visualization of the clusters\n",
        "\n",
        "\n",
        "## Steps\n",
        "1. Create the dataset to store the model \n",
        "1. Examine the training data \n",
        "1. Used the CREATE MODEL statement to create the the K-Means model \n",
        "1. Used the ML.PREDICT function to predict the station cluster. \n",
        "1. Use the model to make data-driven decisions to know which features are the most important to determine the income bracket.\n",
        "\n",
        "\n",
        "#### Execute notebook in Colab\n",
        "<a href=\"https://colab.research.google.com/github/paulycloud/ml_portfolio/blob/main/02_BigQuery_ML/04_london_bike_hires_k_means/index.ipynb\">\n",
        "    <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c640527e06e6"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset is the [`London Bicycle Hires public`](https://cloud.google.com/marketplace/details/greater-london-authority/london-bicycles?filter=solution-type:dataset&id=95374cac-2834-4fa2-a71f-fc033ccb5ce4&_ga=2.18644735.777390235.1667764380-77713112.1665872214&_gac=1.186188379.1667497020.CjwKCAjwzY2bBhB6EiwAPpUpZhvKzZjSLlQgJkUZn1ZcM1bGDnIbpyv8CT3SHbte7N3PEvRI58Vg0BoCLbkQAvD_BwE). The London Bicycle Hires data contains the number of hires of London's Santander Cycle Hire Scheme from 2011 to present. The dataset  has about 24,369,201 rows of data. \n",
        "\n",
        "The data includes: \n",
        "*   start and stop timestamps\n",
        "*   station names \n",
        "*   ride duration\n",
        "\n",
        "\n",
        "The queries in this tutorial use Geography Functions available in [**geospatial analytics**](https://cloud.google.com/bigquery/docs/gis-intro)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install the following packages required to execute this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b4ef9b72d43",
        "outputId": "a5e92684-9348-47ca-cfa7-8cdeee1318a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.3 MB 2.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 36.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 233 kB 10.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 206 kB 22.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 408 kB 47.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 106 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 9.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 33.6 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.34.4 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.10.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q google-cloud-bigquery db-dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eqwLyLiibss"
      },
      "source": [
        "## Project Variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HBUd-MvjhFDR"
      },
      "outputs": [],
      "source": [
        "# Project variables \n",
        "#\n",
        "# These are the project variable used in this ML Model: \n",
        "#\n",
        "\n",
        "PROJECT_ID = \"\" # @param {type:\"string\"}\n",
        "bqml_type = \"kmeans\" # @param {type:\"string\"}\n",
        "BQML_MODEL_NAME = \"bqml_kmeans_london_stations_cluster_model\"\n",
        "job_display_name = BQML_MODEL_NAME + \"_job\"\n",
        "ENDPOINT_NAME = BQML_MODEL_NAME + \"_endpoint\"\n",
        "\n",
        "# datasets\n",
        "BQ_DATASET_NAME = \"03_bqml_k_means_clustering_bike\"\n",
        "sql_create_dataset = f\"\"\"CREATE SCHEMA IF NOT EXISTS {BQ_DATASET_NAME}\"\"\"\n",
        "BQ_PUBLIC_DATASET_HIRES = \"bigquery-public-data.london_bicycles.cycle_hire\"\n",
        "BQ_PUBLIC_DATASET_STATIONS = \"bigquery-public-data.london_bicycles.cycle_stations\"\n",
        "\n",
        "# bucket details\n",
        "BUCKET_NAME = \"bqml_tutorials\"\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}/{bqml_type}/\"\n",
        "OUTPUTBUCKET = f\"gs://bqml_datasets_predictions/{bqml_type}/\"\n",
        "\n",
        "# Region \n",
        "REGION = \"us-central1\" # @param {type: \"string\"} "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You might not be able to use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about <a href=\"https://cloud.google.com/vertex-ai/docs/general/locations\" target=\"_blank\">Vertex AI regions</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "set_service_account",
        "outputId": "831de094-5dfd-41fb-b964-4ee4107595f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}\n",
        "print(SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    else:  # IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "import google.cloud.aiplatform as vertex_ai\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI and BigQuery SDKs for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "toA0pXPnZ2f-"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83859376c893"
      },
      "source": [
        "Create the BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0ab485806b17"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f94734ac9312"
      },
      "source": [
        "Use a helper function for sending queries to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e364dab1d353"
      },
      "outputs": [],
      "source": [
        "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\n",
        "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Input: SQL query, as a string, to execute in BigQuery\n",
        "    Returns the query results as a pandas DataFrame, or error, if any\n",
        "    \"\"\"\n",
        "\n",
        "    # Try dry run before executing query to catch any errors\n",
        "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
        "    bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    # If dry run succeeds without errors, proceed to run query\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    client_result = bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    job_id = client_result.job_id\n",
        "\n",
        "    # Wait for query/job to finish running. then get & return data frame\n",
        "    df = client_result.result().to_arrow().to_pandas()\n",
        "    print(f\"Finished job_id: {job_id}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4a686de97f5"
      },
      "source": [
        "## BigQuery ML Model Training & Validation\n",
        "\n",
        "BigQuery ML (BQML) provides the capability to train ML tabular models, such as classification, regression, forecasting, and matrix factorization, in BigQuery using SQL syntax directly. BigQuery ML uses the scalable infrastructure of BigQuery ML so you don't need to set up additional infrastructure for training or batch serving."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the dataset\n",
        "The first step is to create a BigQuery dataset to store your model. To create your dataset:"
      ],
      "metadata": {
        "id": "7Zs_Zs3cB3O6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "088e3b9577b3"
      },
      "outputs": [],
      "source": [
        "sql_create_dataset = f\"\"\"\n",
        "CREATE SCHEMA IF NOT EXISTS {BQ_DATASET_NAME}\n",
        "OPTIONS (\n",
        "    location = 'europe-west4'\n",
        "  )\n",
        "\"\"\"\n",
        "\n",
        "print(sql_create_dataset)\n",
        "\n",
        "run_bq_query(sql_create_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13b6ce9f8d8b"
      },
      "source": [
        "### Examine the training data\n",
        "The “`london_bicycles`” table contains the data needed. \n",
        "\n",
        "Because **k-means** is an **unsupervised** learning technique, model training does not require labels nor does it require to split the data into training data and evaluation data. The following query compiled the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0916b03610bd"
      },
      "outputs": [],
      "source": [
        "sql_inspect = f\"\"\"\n",
        "WITH\n",
        " hs AS (\n",
        " SELECT\n",
        "   h.start_station_name AS station_name,\n",
        "   IF\n",
        "   (EXTRACT(DAYOFWEEK\n",
        "     FROM\n",
        "       h.start_date) = 1\n",
        "     OR EXTRACT(DAYOFWEEK\n",
        "     FROM\n",
        "       h.start_date) = 7,\n",
        "     \"weekend\",\n",
        "     \"weekday\") AS isweekday,\n",
        "   h.duration,\n",
        "   ST_DISTANCE(ST_GEOGPOINT(s.longitude,\n",
        "       s.latitude),\n",
        "     ST_GEOGPOINT(-0.1,\n",
        "       51.5))/1000 AS distance_from_city_center\n",
        " FROM\n",
        "   {BQ_PUBLIC_DATASET_HIRES} AS h\n",
        " JOIN\n",
        "   {BQ_PUBLIC_DATASET_STATIONS} AS s\n",
        " ON\n",
        "   h.start_station_id = s.id\n",
        " WHERE\n",
        "   h.start_date BETWEEN CAST('2015-01-01 00:00:00' AS TIMESTAMP)\n",
        "   AND CAST('2016-01-01 00:00:00' AS TIMESTAMP) ),\n",
        " stationstats AS (\n",
        " SELECT\n",
        "   station_name,\n",
        "   AVG(duration) AS duration,\n",
        "   COUNT(duration) AS num_trips,\n",
        "   MAX(distance_from_city_center) AS distance_from_city_center\n",
        " FROM\n",
        "   hs\n",
        " GROUP BY\n",
        "   station_name )\n",
        "SELECT\n",
        " *\n",
        "FROM\n",
        " stationstats\n",
        "ORDER BY\n",
        " distance_from_city_center ASC\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "run_bq_query(sql_inspect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f304053600"
      },
      "source": [
        "### Used the CREATE MODEL statement to create the the K-Means model\n",
        "\n",
        "When the model is created, the clustering field is station_name, and cluster the data based on station attribute, for example the distance of the station from the city center.\n",
        "\n",
        "In the `OPTIONS` parameter:\n",
        "* with `model_registry=\"vertex_ai\"`, the BigQuery ML model will automatically be <a href=\"https://cloud.google.com/vertex-ai/docs/model-registry/model-registry-bqml\" target=\"_blank\">registered to Vertex AI Model Registry</a>, which enables you to view all of your registered models and its versions on Google Cloud in one place.\n",
        "\n",
        "* `vertex_ai_model_version_aliases allows you to set aliases to help you keep track of your model version (<a href=\"https://cloud.google.com/vertex-ai/docs/model-registry/model-alias\" target=\"_blank\">documentation</a>).\n",
        "\n",
        "Clustering of bike stations was based on the following attributes:\n",
        "\n",
        "- Duration of rentals\n",
        "- Number of trips per day\n",
        "- Distance from city center\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "414c45011c1f"
      },
      "outputs": [],
      "source": [
        "# this cell may take ~1 min to run\n",
        "\n",
        "sql_train_model_bqml = f\"\"\"\n",
        "\n",
        "CREATE\n",
        "OR REPLACE MODEL {BQ_DATASET_NAME}.{BQML_MODEL_NAME} OPTIONS(\n",
        "    model_type = 'kmeans',\n",
        "    num_clusters = 4,\n",
        "    model_registry = \"vertex_ai\",\n",
        "    vertex_ai_model_version_aliases = ['kmeans', 'experimental']\n",
        ") AS WITH hs AS (\n",
        "    SELECT\n",
        "        h.start_station_name AS station_name,\n",
        "        IF (\n",
        "            EXTRACT(\n",
        "                DAYOFWEEK\n",
        "                FROM\n",
        "                    h.start_date\n",
        "            ) = 1\n",
        "            OR EXTRACT(\n",
        "                DAYOFWEEK\n",
        "                FROM\n",
        "                    h.start_date\n",
        "            ) = 7,\n",
        "            \"weekend\",\n",
        "            \"weekday\"\n",
        "        ) AS isweekday,\n",
        "        h.duration,\n",
        "        ST_DISTANCE(\n",
        "            ST_GEOGPOINT(s.longitude, s.latitude),\n",
        "            ST_GEOGPOINT(-0.1, 51.5)\n",
        "        ) / 1000 AS distance_from_city_center\n",
        "    FROM\n",
        "        { BQ_PUBLIC_DATASET_HIRES } AS h\n",
        "        JOIN { BQ_PUBLIC_DATASET_STATIONS } AS s ON h.start_station_id = s.id\n",
        "    WHERE\n",
        "        h.start_date BETWEEN CAST('2015-01-01 00:00:00' AS TIMESTAMP)\n",
        "        AND CAST('2016-01-01 00:00:00' AS TIMESTAMP)\n",
        "),\n",
        "stationstats AS (\n",
        "    SELECT\n",
        "        station_name,\n",
        "        isweekday,\n",
        "        AVG(duration) AS duration,\n",
        "        COUNT(duration) AS num_trips,\n",
        "        MAX(distance_from_city_center) AS distance_from_city_center\n",
        "    FROM\n",
        "        hs\n",
        "    GROUP BY\n",
        "        station_name,\n",
        "        isweekday\n",
        ")\n",
        "SELECT\n",
        "    *\n",
        "EXCEPT\n",
        "(station_name, isweekday)\n",
        "FROM\n",
        "    stationstats\n",
        "\"\"\"\n",
        "\n",
        "print(sql_train_model_bqml)\n",
        "\n",
        "run_bq_query(sql_train_model_bqml)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results \n",
        "\n",
        "The results matrix show the *Davies-Bouldin Index* and the *Mean Squared Distance.*\n",
        "\n",
        "**Davies-Bouldin index** is a validation metric that is often used in order to evaluate the optimal number of clusters to use. It is defined as a ratio between the cluster scatter and the cluster’s separation and a lower value will mean that the clustering is better.\n",
        "\n",
        "The **mean squared distance** makes reference to the intra cluster variance, which we want to minimize as a lower WCSS (within-cluster sums of squares) will maximize the distance between clusters.\n",
        "\n",
        "The Numerical Features tab displays visualizations of the clusters identified by the k-means model. Under Numerical features, bar graphs display up to 10 of the most important numerical feature values for each centroid.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_VOr_3x8q_p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Vo2Xyupcmd"
      },
      "source": [
        "### Step 4: Used the ML.PREDICT function to predict the station cluster.\n",
        "\n",
        "The ML.PREDICT function was used to predict the cluster for a given set of stations. You predict clusters for all station names that contain the string Kennington."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m9FCEEFpdPq"
      },
      "outputs": [],
      "source": [
        "sql_ml_predict = f\"\"\"\n",
        " WITH\n",
        " hs AS (\n",
        " SELECT\n",
        "   h.start_station_name AS station_name,\n",
        "   IF\n",
        "   (EXTRACT(DAYOFWEEK\n",
        "     FROM\n",
        "       h.start_date) = 1\n",
        "     OR EXTRACT(DAYOFWEEK\n",
        "     FROM\n",
        "       h.start_date) = 7,\n",
        "     \"weekend\",\n",
        "     \"weekday\") AS isweekday,\n",
        "   h.duration,\n",
        "   ST_DISTANCE(ST_GEOGPOINT(s.longitude,\n",
        "       s.latitude),\n",
        "     ST_GEOGPOINT(-0.1,\n",
        "       51.5))/1000 AS distance_from_city_center\n",
        " FROM\n",
        "   {BQ_PUBLIC_DATASET_HIRES} AS h\n",
        " JOIN\n",
        "   {BQ_PUBLIC_DATASET_STATIONS} AS s\n",
        " ON\n",
        "   h.start_station_id = s.id\n",
        " WHERE\n",
        "   h.start_date BETWEEN CAST('2015-01-01 00:00:00' AS TIMESTAMP)\n",
        "   AND CAST('2016-01-01 00:00:00' AS TIMESTAMP) ),\n",
        " stationstats AS (\n",
        " SELECT\n",
        "   station_name,\n",
        "   AVG(duration) AS duration,\n",
        "   COUNT(duration) AS num_trips,\n",
        "   MAX(distance_from_city_center) AS distance_from_city_center\n",
        " FROM\n",
        "   hs\n",
        " GROUP BY\n",
        "   station_name )\n",
        "SELECT\n",
        " * EXCEPT(nearest_centroids_distance)\n",
        "FROM\n",
        " ML.PREDICT( MODEL {BQ_DATASET_NAME}.{BQML_MODEL_NAME},\n",
        "   (\n",
        "   SELECT\n",
        "     *\n",
        "   FROM\n",
        "     stationstats\n",
        "   WHERE\n",
        "     REGEXP_CONTAINS(station_name, 'Kennington')))\n",
        "\"\"\"\n",
        "\n",
        "print(sql_ml_predict)\n",
        "\n",
        "run_bq_query(sql_ml_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e806ebc48a2"
      },
      "source": [
        "### Use the ML.EXPLAIN_PREDICT function to know which features are the most important to determine the weight.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d31605829283"
      },
      "source": [
        "To understand why the model is generating these prediction results, you can use the ML.EXPLAIN_PREDICT function.\n",
        "\n",
        "\n",
        "<a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-explain-predict\" target=\"_blank\">ML.EXPLAIN_PREDICT</a> has built-in <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-xai-overview\" target=\"_blank\">Explainable AI</a>. This allows you to see the top contributing features to each prediction and interpret how it was computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d500fdbfb44"
      },
      "outputs": [],
      "source": [
        "sql_explain_predict = f\"\"\"\n",
        "\n",
        "SELECT * FROM\n",
        "ML.EXPLAIN_PREDICT(MODEL {BQ_DATASET_NAME}.{BQML_MODEL_NAME},\n",
        " (\n",
        "  WITH\n",
        " hs AS (\n",
        " SELECT\n",
        "   h.start_station_name AS station_name,\n",
        "   IF\n",
        "   (EXTRACT(DAYOFWEEK\n",
        "     FROM\n",
        "       h.start_date) = 1\n",
        "     OR EXTRACT(DAYOFWEEK\n",
        "     FROM\n",
        "       h.start_date) = 7,\n",
        "     \"weekend\",\n",
        "     \"weekday\") AS isweekday,\n",
        "   h.duration,\n",
        "   ST_DISTANCE(ST_GEOGPOINT(s.longitude,\n",
        "       s.latitude),\n",
        "     ST_GEOGPOINT(-0.1,\n",
        "       51.5))/1000 AS distance_from_city_center\n",
        " FROM\n",
        "   {BQ_PUBLIC_DATASET_HIRES} AS h\n",
        " JOIN\n",
        "   {BQ_PUBLIC_DATASET_STATIONS} AS s\n",
        " ON\n",
        "   h.start_station_id = s.id\n",
        " WHERE\n",
        "   h.start_date BETWEEN CAST('2015-01-01 00:00:00' AS TIMESTAMP)\n",
        "   AND CAST('2016-01-01 00:00:00' AS TIMESTAMP) ),\n",
        " stationstats AS (\n",
        " SELECT\n",
        "   station_name,\n",
        "   AVG(duration) AS duration,\n",
        "   COUNT(duration) AS num_trips,\n",
        "   MAX(distance_from_city_center) AS distance_from_city_center\n",
        " FROM\n",
        "   hs\n",
        " GROUP BY\n",
        "   station_name )\n",
        "SELECT\n",
        " * EXCEPT(nearest_centroids_distance)\n",
        " )\n",
        "\"\"\"\n",
        "\n",
        "print(sql_explain_predict)\n",
        "\n",
        "run_bq_query(sql_explain_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc0c1c1b03f9"
      },
      "source": [
        "### Inspect the model on Vertex AI Model Registry\n",
        "\n",
        "When the model was trained in BigQuery ML, the line `model_registry=\"vertex_ai\"` registered the model to Vertex AI Model Registry automatically upon completion.\n",
        "\n",
        "You can view the model on the <a href=\"https://console.cloud.google.com/vertex-ai/models\" target=\"_blank\">Vertex AI Model Registry page</a>, or use the code below to check that it was successfully registered:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b00664302839"
      },
      "outputs": [],
      "source": [
        "model = vertex_ai.Model(model_name=BQML_MODEL_NAME)\n",
        "\n",
        "print(model.gca_resource)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89455f708f54"
      },
      "source": [
        "### Deploy the model to an endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6120dcc1ff6"
      },
      "source": [
        "While BigQuery ML supports batch prediction with <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict\" target=\"_blank\">ML.PREDICT</a> and <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-explain-predict\" target=\"_blank\">ML.EXPLAIN_PREDICT</a>, BigQuery ML is not suitable for real-time predictions where you need low latency predictions with potentially high frequency of requests.\n",
        "\n",
        "In other words, deploying the BigQuery ML model to an endpoint enables you to do online predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ab7e1ac83c"
      },
      "source": [
        "#### Create a Vertex AI endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a61ea55f685"
      },
      "source": [
        "To deploy your model to an endpoint, you will first need to create an endpoint before you deploy the model to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7d80941cf30"
      },
      "outputs": [],
      "source": [
        "\n",
        "endpoint = vertex_ai.Endpoint.create(\n",
        "    display_name=ENDPOINT_NAME,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "print(endpoint.display_name)\n",
        "print(endpoint.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b58a104207d2"
      },
      "source": [
        "#### List endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "951ed1693f6b"
      },
      "source": [
        "List the endpoints to make sure it has successfully been created. (You can also view your endpoints on the <a href=\"https://console.cloud.google.com/vertex-ai/endpoints\" target=\"_blank\">Vertex AI Endpoints page</a>)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4d7707910bc"
      },
      "outputs": [],
      "source": [
        "endpoint.list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba0d40b26cfb"
      },
      "source": [
        "#### Deploy model to Vertex endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a90be5b77a2"
      },
      "source": [
        "With the new endpoint, you can now deploy your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c70ecc568ee5"
      },
      "outputs": [],
      "source": [
        "# deploying the model to the endpoint may take 10-15 minutes\n",
        "model.deploy(endpoint=endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c303d779477b"
      },
      "source": [
        "You can also check on the status of your model by visiting the <a href=\"https://console.cloud.google.com/vertex-ai/endpoints\" target=\"_blank\">Vertex AI Endpoints page</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb04d49c6f1"
      },
      "source": [
        "### Make online predictions to the endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a888784de7b"
      },
      "source": [
        "Using a sample of the training data, you can test the endpoint to make online predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f17b049a846c"
      },
      "outputs": [],
      "source": [
        "df_sample_requests_list = [\n",
        "    {\n",
        "        \"age\": 45,\n",
        "        \"workclass\": \"Private\",\n",
        "        \"marital_status\": \"Single\",\n",
        "        \"education_num\": 6,\n",
        "        \"occupation\": \"Exec-managerial\",\n",
        "        \"hours_per_week\": 40,\n",
        "    },\n",
        "    {\n",
        "        \"age\": 30,\n",
        "        \"workclass\": \"Private\",\n",
        "        \"marital_status\": \"Married\",\n",
        "        \"education_num\": 2,\n",
        "        \"occupation\": \"Machine-op-inspct\",\n",
        "        \"hours_per_week\": 50,\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4839f31d2f8"
      },
      "outputs": [],
      "source": [
        "prediction = endpoint.predict(df_sample_requests_list)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "347ffda86396"
      },
      "source": [
        "You can then extract the predictions from the prediction response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea659b7c178d"
      },
      "outputs": [],
      "source": [
        "prediction.predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can <a href=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects\" target=\"_blank\">delete the Google Cloud\n",
        "project</a> you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Undeploy model from endpoint and delete endpoint\n",
        "endpoint.undeploy_all()\n",
        "endpoint.delete()\n",
        "\n",
        "# Delete BigQuery dataset, including the BigQuery ML model\n",
        "! bq rm -r -f $PROJECT_ID:$BQ_DATASET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}