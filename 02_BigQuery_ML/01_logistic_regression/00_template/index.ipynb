{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# How to use a logistic classification model to predict Income bucket on census data using  BigQueryML\n",
        "\n",
        "###### Copyright\n",
        "**GUI Author:** [ Google Cloud](https://cloud.google.com/bigquery-ml/docs/logistic-regression-prediction)<br>\n",
        "**Notebook Adoption Author:** [Paul Kamau](https://paulkamau.com)<br>\n",
        "**Project Type:** BQML Logistic Reg<br>\n",
        "**Date created:** 2022/12/10<br>\n",
        "**Training Dataset** This project uses the public dataset[`census_adult_income`](https://cloud.google.com/bigquery?p=bigquery-public-data&d=census_bureau_usa&page=dataset) has about 32,561 rows of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Summary\n",
        "The pupose of this project is to use a binary logistic regression model in BigQuery ML to predict the income range of respondents in the US Census Dataset. The model will predict whether a US Census respondent's income falls into one of two ranges **(> 50K or < 50K a year)** based on the respondent's demographic attributes.\n",
        "\n",
        "### Dataset\n",
        "The public dataset [`census_adult_income`](https://cloud.google.com/bigquery?p=bigquery-public-data&d=census_bureau_usa&page=dataset) has about 32,561 rows of data.\n",
        "\n",
        "This tutorial uses the following Google Cloud data analytics and ML services:\n",
        "\n",
        "### Key Concepts\n",
        "- BigQuery\n",
        "- BigQuery ML\n",
        "- Dimensionality Reduction\n",
        "- Vertex AI Model Registry\n",
        "- Vertex endpoints\n",
        "- Logistic regression\n",
        "- Explainable AI\n",
        "- ML.EVALUATE\n",
        "- ML PREDICT\n",
        "\n",
        "### Steps\n",
        "1. Create the dataset\n",
        "1. Use the SELECT statement to examine the data\n",
        "1. Use the CREATE VIEW statement to compile your training data\n",
        "1. Use the CREATE MODEL statement to create your logistic regression model.\n",
        "1. Use the ML.EVALUATE function to evaluate the model data\n",
        "1. Use the ML.PREDICT function to predict the income bracket for a given set of census participants.\n",
        "1. Use the ML.EXPLAIN_PREDICT function to explain prediction results with explainable AI Methods.\n",
        "1. Use the ML.GLOBAL_EXPLAIN function to know which features are the most important to determine the income bracket.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfuPUmwcqLXX"
      },
      "source": [
        "### What is Logisitc Regression?\n",
        "\n",
        "Logistic regression is a **supervised** **classification algorithm** used to assign observations to a discrete set of classes.\n",
        "\n",
        "For example, a retailer may want to predict whether a given customer will purchase a new product, based on other information about that customer. In that case, the two labels might be **\"will buy\"** and **\"won't buy.\"**\n",
        "\n",
        "You can construct your dataset such that one column represents the **label**. The data you can use to train such a binary logistic regression model include the customer's location, their previous purchases, the customer's reported preferences, and so on.\n",
        "\n",
        "**Types of logistic regression**\n",
        "- Binary (Pass/Fail) *This tutorial as an example of one.*\n",
        "- Multi (Cats, Dogs, Sheep)\n",
        "- Ordinal (Low, Medium, High)\n",
        "\n",
        "In Distiction:\n",
        "\n",
        "**Linear Regression** is a supervised machine learning algorithm where the predicted output is **continuous** and has a constant slope. It’s used to predict values within a continuous range, **(e.g. sales, price)** rather than trying to classify them into categories (e.g. cat, dog)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Prepare the Envrionment with Package Installs\n",
        "1. Install the following packages required to execute this notebook.\n",
        "1. install the workbench notebook product dependencies\n",
        "1. Automatically restart kernel after installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import AI Platform and ML libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q google-cloud-bigquery db-dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "#### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vshsBiYRCEv"
      },
      "source": [
        "### Prepare the notebook to run in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRfl91PPTPcQ"
      },
      "source": [
        "#### Import the Google Cloud SDK packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as vertex_ai\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "from typing import Union  # This module provides runtime support for type hints\n",
        "from google.cloud import bigquery\n",
        "from sklearn import metrics\n",
        "from google.api import httpbody_pb2\n",
        "\n",
        "print(\"All imports complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eqwLyLiibss"
      },
      "source": [
        "#### GCP & BQML Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBUd-MvjhFDR"
      },
      "outputs": [],
      "source": [
        "# Project variables\n",
        "#\n",
        "# These are the project variable used in this ML Model:\n",
        "#\n",
        "\n",
        "PROJECT_ID = \"\" # @param {type:\"string\"}\n",
        "BQML_MODEL_TYPE = \"LOGISTIC_REG\" #@param [\"LINEAR_REG\", \"LOGISTIC_REG\",\"KMEANS\",\"PCA\",\"MATRIX_FACTORIZATION\",\"AUTOENCODER\",\"AUTOML_REGRESSOR\", \"AUTOML_CLASSIFIER\",\"BOOSTED_TREE_CLASSIFIER\",\"BOOSTED_TREE_REGRESSOR\",\"RANDOM_FOREST_CLASSIFIER\",\"RANDOM_FOREST_REGRESSOR\", \"DNN_CLASSIFIER\",\"DNN_REGRESSOR\",\"DNN_LINEAR_COMBINED_CLASSIFIER\", \"DNN_LINEAR_COMBINED_REGRESSOR\",\"ARIMA_PLUS\",\"TENSORFLOW\"]\n",
        "\n",
        "BQML_MODEL_NAME = \"bqml_log_reg_census_income_predict_model\"\n",
        "BQML_JOB_DISPLAY_NAME = BQML_MODEL_NAME + \"_job\"\n",
        "BQML_ENDPOINT_NAME = BQML_MODEL_NAME + \"_endpoint\"\n",
        "\n",
        "# datasets\n",
        "BQ_DATASET_NAME = BQML_MODEL_NAME + \"_dataset\"\n",
        "BQ_PUBLIC_DATASET = \"bigquery-public-data.ml_datasets.census_adult_income\"\n",
        "BQ_DATASET_VIEW_TABLE= f\"{BQ_DATASET_NAME}.input_view\"\n",
        "\n",
        "# bucket details\n",
        "BUCKET_NAME = \"bqml_tutorials\"\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}/{BQML_MODEL_TYPE}/\"\n",
        "OUTPUTBUCKET = f\"gs://bqml_datasets_predictions/{BQML_MODEL_TYPE}/\"\n",
        "\n",
        "# Region\n",
        "REGION = \"us-central1\" #@param [\"us-central1\", \"europe-west4\", \"asia-east1\"]\n",
        "\n",
        "print(\"All variables set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGBTBZLnflWD"
      },
      "source": [
        "#### Set the Service account automatically from gcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "# Get your service account from gcloud\n",
        "if not IS_COLAB:\n",
        "    shell_output = !gcloud auth list 2>/dev/null\n",
        "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "else:  # IS_COLAB:\n",
        "    shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "    project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "    SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "#### Initialize Vertex AI and BigQuery SDKs for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toA0pXPnZ2f-"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83859376c893"
      },
      "source": [
        "Create the BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ab485806b17"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f94734ac9312"
      },
      "source": [
        "Use a helper function for sending queries to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e364dab1d353"
      },
      "outputs": [],
      "source": [
        "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\n",
        "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Input: SQL query, as a string, to execute in BigQuery\n",
        "    Returns the query results as a pandas DataFrame, or error, if any\n",
        "    \"\"\"\n",
        "\n",
        "    # Try dry run before executing query to catch any errors\n",
        "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
        "    bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    # If dry run succeeds without errors, proceed to run query\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    client_result = bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    job_id = client_result.job_id\n",
        "\n",
        "    # Wait for query/job to finish running. then get & return data frame\n",
        "    df = client_result.result().to_arrow().to_pandas()\n",
        "    print(f\"Finished job_id: {job_id}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4a686de97f5"
      },
      "source": [
        "# Using BigQuery ML logistic regression model for income bucket classification on census data\n",
        "\n",
        "BigQuery ML (BQML) provides the capability to train ML tabular models, such as classification, regression, forecasting, and matrix factorization, in BigQuery using SQL syntax directly. BigQuery ML uses the scalable infrastructure of BigQuery ML so you don't need to set up additional infrastructure for training or batch serving.\n",
        "\n",
        "This entire demo takes less than 5 minutes to execute from start to finish. The goal however isn't to just run sql queries but to hone into the reason behind the results and how to optimize for the best outcome. Its about the patterns of thinking to apply when running through this. This is where the value really exists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkY7wi12WVAm"
      },
      "source": [
        "#### **Commentary**:\n",
        "The public dataset we're using for this demo is the [`census_adult_income`](https://console.cloud.google.com/marketplace/product/united-states-census-bureau/us-geographic-boundaries?project=paulkamau) has about 32,561 rows of data. We will import that data and work with it inside our own datasets.\n",
        "\n",
        "Additionally, you can find datasets in these locations depending on your use case:\n",
        "\n",
        "1. [TensorFlow Datasets](https://www.tensorflow.org/datasets)\n",
        "1. [Google Dataset Search](https://datasetsearch.research.google.com/)\n",
        "1. [Google Cloud Marketplace Datasets](https://console.cloud.google.com/marketplace/browse?project=paulkamau)\n",
        "1. [UCI](https://archive.ics.uci.edu/ml/datasets.php)\n",
        "1. [Kaggle](https://www.kaggle.com/datasets?search=transaction)\n",
        "1. [OpenML](https://www.openml.org/search?type=data&status=active&sort=runs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTT96Vy2poGB"
      },
      "source": [
        "### Step 1: Create Your Dataset\n",
        "BigQuery organizes data tables into units called datasets. These datasets are scoped to your GCP project. Datasets are:\n",
        "\n",
        "- Collections of “related” tables/views together with labels and description\n",
        "- Allow storage access control at Dataset level\n",
        "- Define location of data i.e. multi-regional (US, EU) or regional (asia-northeast1)\n",
        "\n",
        "[Dataset SDK Reference](https://cloud.google.com/bigquery/docs/datasets-intro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me9upiCPXqXc"
      },
      "outputs": [],
      "source": [
        "# check if Dataset exists.\n",
        "sql_list_dataset_exists = f\"\"\"SELECT count (*) FROM INFORMATION_SCHEMA.SCHEMATA WHERE schema_name = '{BQ_DATASET_NAME}'\"\"\"\n",
        "\n",
        "# Display the sql statement\n",
        "print(sql_list_dataset_exists)\n",
        "\n",
        "# execute the sql statement\n",
        "run_bq_query(sql_list_dataset_exists)\n",
        "\n",
        "print(f'Review the Dataset in the Console:\\nhttps://console.cloud.google.com/bigquery?page=dataset&p={PROJECT_ID}&d={BQ_DATASET_NAME}&project={PROJECT_ID}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "088e3b9577b3"
      },
      "outputs": [],
      "source": [
        "sql_create_dataset = f\"\"\"CREATE SCHEMA IF NOT EXISTS {BQ_DATASET_NAME}\"\"\"\n",
        "\n",
        "print(sql_create_dataset)\n",
        "\n",
        "run_bq_query(sql_create_dataset)\n",
        "\n",
        "print(f'Review the Dataset in the Console:\\nhttps://console.cloud.google.com/bigquery?page=dataset&p={PROJECT_ID}&d={BQ_DATASET_NAME}&project={PROJECT_ID}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49dd00d5fbe5"
      },
      "source": [
        "Inspect data that has been pre-processed from  [`census_adult_income`](https://cloud.google.com/bigquery?p=bigquery-public-data&d=census_bureau_usa&page=dataset) so that it can be used for classification. (32,561 rows)\n",
        "\n",
        "The data view results show that the **``income_bracket``** column in the census_adult_income table has only one of two values: <=50K or >50K.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAeVXLUXbhIq"
      },
      "outputs": [],
      "source": [
        "sql_inspect_full_feature_dataset = f\"\"\"\n",
        "SELECT *\n",
        "FROM\n",
        " `{BQ_PUBLIC_DATASET}`\n",
        "LIMIT\n",
        " 10;\n",
        "\"\"\"\n",
        "\n",
        "sql_inspect_record_count_dataset = f\"\"\"\n",
        "SELECT count(*)\n",
        "FROM\n",
        " `{BQ_PUBLIC_DATASET}`;\n",
        "\"\"\"\n",
        "\n",
        "# Display the sql statement\n",
        "print(sql_inspect_full_feature_dataset)\n",
        "\n",
        "run_bq_query(sql_inspect_full_feature_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRt6l0Noeas"
      },
      "source": [
        "#### **Commentary**:\n",
        "The SQL above brings back all the features (columns) in the dataset. Not every column is going to be useful for our modeling.\n",
        "\n",
        "**Data preparation **is required to remove duplicates, unecessary data or noise from the dataset.\n",
        "\n",
        "The final columns (features) used for prediction are called **inputs**.\n",
        "\n",
        "The prediction column, in this case, `income bracket`, is called **Label**. This is the purpose of this logistic classifiaction model.\n",
        "\n",
        "#### **Questions**:\n",
        "- How do you know that it was the right call to drop these columns?\n",
        "- What are some training considerations to make?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0916b03610bd"
      },
      "outputs": [],
      "source": [
        "sql_inspect_dataset = f\"\"\"\n",
        "SELECT\n",
        " age,\n",
        " workclass,\n",
        " marital_status,\n",
        " education_num,\n",
        " occupation,\n",
        " hours_per_week,\n",
        " income_bracket\n",
        "FROM\n",
        " `{BQ_PUBLIC_DATASET}`\n",
        "LIMIT\n",
        " 10;\n",
        "\"\"\"\n",
        "run_bq_query(sql_inspect_dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN15xrZSnZHM"
      },
      "source": [
        "\n",
        "#### **Commentary**:\n",
        "\n",
        "The number of **input variables** or **features** for a dataset is referred to as its dimensionality. *(See all the columns from the sql above)*\n",
        "\n",
        "**Dimensionality reduction** refers to techniques that reduce the number of input variables in a dataset (See all the columns from the sql below)\n",
        "\n",
        "- age,\n",
        "- workclass,\n",
        "- marital_status,\n",
        "- education_num,\n",
        "- occupation,\n",
        "- hours_per_week,\n",
        "- income_bracket\n",
        "\n",
        "So what we're saying is the columns we've either dropped or consolidated have no value to our model prediction. These are the columns we've dropped;\n",
        "\n",
        "- Dropped columns are:\n",
        "- Functional weight,\n",
        "- relationship,\n",
        "- Education\n",
        "- race,\n",
        "- sex,\n",
        "- capital_gain,\n",
        "- capital_loss,\n",
        "- native_country,\n",
        "\n",
        "You can experiment with different numbers of columns to determine if your subsequent models are more precise, run faster, etc. Starting with a small feature set is a good way to go about it\n",
        "\n",
        "#### **Questions**:\n",
        "- How do you know that it was the right call to drop these columns?\n",
        "- What are some training considerations to make?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAMRjx_wkj-D"
      },
      "source": [
        "### Step 2: Use the CREATE VIEW statement to compile the training data\n",
        "\n",
        "The next step was to create a view that compiles the training data. This was done by selecting the data used to train your logistic regression model.\n",
        "\n",
        "A **view** is a virtual table defined by a SQL query. When you create a view, you query it in the same way you query a table. When a user queries the view, the query results contain data only from the tables and fields specified in the query that defines the view.\n",
        "\n",
        "The census respondent income prediction is done based on the following attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctpaMNZxkkjx"
      },
      "outputs": [],
      "source": [
        "# Create the dataset view if it doesn't exist\n",
        "sql_create_dataset_view_bqml = f\"\"\"\n",
        "CREATE VIEW IF NOT EXISTS {BQ_DATASET_NAME}.input_view AS\n",
        "SELECT\n",
        " age,\n",
        " workclass,\n",
        " marital_status,\n",
        " education_num,\n",
        " occupation,\n",
        " hours_per_week,\n",
        " income_bracket,\n",
        " CASE\n",
        "   WHEN MOD(functional_weight, 10) < 8 THEN 'training'\n",
        "   WHEN MOD(functional_weight, 10) = 8 THEN 'evaluation'\n",
        "   WHEN MOD(functional_weight, 10) = 9 THEN 'prediction'\n",
        " END AS dataframe\n",
        "FROM\n",
        "`{BQ_PUBLIC_DATASET}`\n",
        "\"\"\"\n",
        "\n",
        "run_bq_query(sql_create_dataset_view_bqml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0uhuQjSwPxP"
      },
      "outputs": [],
      "source": [
        "# We can examine the view data using the sql below\n",
        "sql_examine_view_data = f\"\"\"\n",
        "SELECT *\n",
        "FROM {BQ_DATASET_NAME}.input_view;\n",
        "\"\"\"\n",
        "\n",
        "run_bq_query(sql_examine_view_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "566f3395f20b"
      },
      "source": [
        "### Step 3: Use the CREATE MODEL statement to create your logistic regression model.\n",
        "\n",
        "The query below trains a logistic regression model using BigQuery ML. BigQuery resources are used to train the model.\n",
        "\n",
        "View the **BQML model** [doc](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create) reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxKDUCkL1ZZh"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Let's check if the model already exists.\n",
        "# Note: We're tapping into the python notebook's integration with vertex without invoking sql in any way.\n",
        "#\n",
        "model = vertex_ai.Model(model_name=BQML_MODEL_NAME)\n",
        "\n",
        "print(model.gca_resource)\n",
        "\n",
        "print(f'Review the Model in the Console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{BQML_MODEL_NAME}?project={PROJECT_ID}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "414c45011c1f"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Create the training job. this cell may take ~1 min to run\n",
        "#\n",
        "\n",
        "sql_train_bqml_model_job = f\"\"\"\n",
        "CREATE MODEL IF NOT EXISTS {BQ_DATASET_NAME}.{BQML_MODEL_NAME} OPTIONS (\n",
        "  model_type = \"{BQML_MODEL_TYPE}\",\n",
        "  auto_class_weights = TRUE,\n",
        "  input_label_cols = ['income_bracket'],\n",
        "  model_registry = \"vertex_ai\",\n",
        "  early_stop = TRUE,\n",
        "  vertex_ai_model_version_aliases = ['logistic_reg', 'experimental']\n",
        ") AS\n",
        "SELECT\n",
        "  *\n",
        "EXCEPT\n",
        "(dataframe)\n",
        "FROM\n",
        "  {BQ_DATASET_VIEW_TABLE} --The SELECT statement queries the view from Step 3.\n",
        "WHERE\n",
        "  dataframe = 'training' --since we're using 80% of our data for training\n",
        "\"\"\"\n",
        "\n",
        "print(sql_train_bqml_model_job)\n",
        "\n",
        "run_bq_query(sql_train_bqml_model_job)\n",
        "\n",
        "print(f'Review the Model in the Console:\\nhttps://console.cloud.google.com/bigquery?referrer=search&project={PROJECT_ID}&ws=!1m5!1m4!5m3!1s{PROJECT_ID}!2s{BQ_DATASET_NAME}!3s{BQML_MODEL_NAME}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y8avO0mtVkC"
      },
      "source": [
        "In the `OPTIONS` parameter:\n",
        "* with `model_registry=\"vertex_ai\"`, the BigQuery ML model will automatically be <a href=\"https://cloud.google.com/vertex-ai/docs/model-registry/model-registry-bqml\" target=\"_blank\">registered to Vertex AI Model Registry</a>, which enables you to view all of your registered models and its versions on Google Cloud in one place.\n",
        "- `auto_class_weights` = TRUE, this Balances the class labels in the training data\n",
        "\n",
        "- input_label_cols = ['income_bracket'], This is the prediction column our model should figure out. A binary option of >50 or < 50\n",
        "\n",
        "* `vertex_ai_model_version_aliases allows you to set aliases to help you keep track of your model version [Doc](https://cloud.google.com/vertex-ai/docs/model-registry/model-alias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzn3TAualqln"
      },
      "source": [
        "### Step3b. Use the ML.TRAINING_INFO to see the training iterations and details of the model\n",
        "\n",
        "The ML.TRAINING_INFO [function](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-train) allows you to see information about the training iterations of a model. ML.TRAINING_INFO can be run while the CREATE MODEL query is running, or after it is run. If you run a query that contains ML.TRAINING_INFO before the first training iteration is complete, the query returns a Not found error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ0I5tp_l8X5"
      },
      "outputs": [],
      "source": [
        "# Here's where we're using the bq client because of the additional features to tap into matplotlib.\n",
        "\n",
        "sql_training_info_model = bq_client.query(\n",
        "    query = f\"\"\"\n",
        "        SELECT *\n",
        "        FROM ML.TRAINING_INFO(MODEL `{PROJECT_ID}.{BQ_DATASET_NAME}.{BQML_MODEL_NAME}`)\n",
        "        ORDER BY iteration\n",
        "        \"\"\"\n",
        ").to_dataframe()\n",
        "\n",
        "sql_training_info_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zXUWi4vmdie"
      },
      "source": [
        "\n",
        "#### **Commentary**:\n",
        "We can observe that the model training had 7 Iterations (starting at 0)\n",
        "\n",
        "**Training run**\n",
        "\n",
        "The value in this column is zero for a newly created model. If you retrain the model using warm_start (new training data), this value is incremented.\n",
        "\n",
        "**iteration**\n",
        "\n",
        "The iteration number of the training run. its how many times the model algorithm has attempted to optimize the loss. The value for the first iteration is zero. This value is incremented for each additional training run.\n",
        "\n",
        "Note: Iterations and epochs are not the same.  *Iterations* is the number of batches (size of data passed) needed to complete one epoch. One Epoch is when an **ENTIRE** dataset is passed forward and backward through the neural network only ONCE.\n",
        "\n",
        "**loss**\n",
        "\n",
        "A *loss function* is a function that compares the target and predicted output values; measures how well the neural network models the training data. When training, we aim to minimize this loss between the predicted and target outputs.\n",
        "\n",
        "**eval_loss**\n",
        "\n",
        "The loss metric calculated on the holdout data.\n",
        "\n",
        "**Learning_rate**\n",
        "\n",
        "a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.\n",
        "\n",
        "**duration_ms** How long the iteration took, in milliseconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjNlukD_uNSN"
      },
      "source": [
        "### Step3c. Let's take a look at the Loss History\n",
        "\n",
        "Y axis = loss (high at the beginning. goal is to bring it low)\n",
        "\n",
        "X axis = eval_loss ()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZG9MpNpo6EZ"
      },
      "outputs": [],
      "source": [
        "sql_training_info_model.plot(x = 'iteration', y = ['loss','eval_loss'], figsize = (10, 5), grid = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aaaae772f67"
      },
      "source": [
        "### Step 4: Use the ML.EVALUATE function to evaluate the model data\n",
        "The ML.EVALUATE function is used to evaluate model metrics. It reports the precision, recall, accuracy, f1_score, log_loss * roc_auc\n",
        "\n",
        "Machine learning model evaluation metrics are used to\n",
        "\n",
        "1. assess quality of fit between the model and the data,\n",
        "2. compare different models, and\n",
        "3. in the context of model selection, and to predict how accurate each model can be expected to perform on a specific data set.\n",
        "\n",
        "Read the **BQML ML EVALUATE function** [doc](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate) reference\n",
        "\n",
        "With the model created, you can now evaluate the logistic regression model. Behind the scenes, BigQuery ML automatically <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#data_split_method\" target=\"_blank\">split the data</a>, which makes it easier to quickly train and evaluate models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1f8ac93d570"
      },
      "outputs": [],
      "source": [
        "sql_evaluate_model = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.EVALUATE(MODEL {BQ_DATASET_NAME}.{BQML_MODEL_NAME})\n",
        "\"\"\"\n",
        "\n",
        "print(sql_evaluate_model)\n",
        "\n",
        "run_bq_query(sql_evaluate_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLj69y0Iys8f"
      },
      "source": [
        "Another Query for getting evaluation metrics broken out by test, validate and training sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ0Hh1ZtvDXz"
      },
      "outputs": [],
      "source": [
        "sql_evaluate_model_all_splits = f\"\"\"\n",
        "SELECT 'TEST' as SPLIT, * FROM ML.EVALUATE(MODEL {BQ_DATASET_NAME}.{BQML_MODEL_NAME},\n",
        "    (SELECT * FROM {BQ_DATASET_VIEW_TABLE} WHERE dataframe = 'evaluation'))\n",
        "UNION ALL\n",
        "SELECT 'VALIDATE' as SPLIT, * FROM ML.EVALUATE(MODEL {BQ_DATASET_NAME}.{BQML_MODEL_NAME},\n",
        "    (SELECT * FROM {BQ_DATASET_VIEW_TABLE} WHERE dataframe = 'prediction'))\n",
        "UNION ALL\n",
        "SELECT 'TRAIN' as SPLIT, * FROM ML.EVALUATE(MODEL {BQ_DATASET_NAME}.{BQML_MODEL_NAME},\n",
        "    (SELECT * FROM {BQ_DATASET_VIEW_TABLE} WHERE dataframe = 'training'))\n",
        "\"\"\"\n",
        "\n",
        "print(sql_evaluate_model_all_splits)\n",
        "\n",
        "run_bq_query(sql_evaluate_model_all_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9f807a50f38"
      },
      "source": [
        "\n",
        "#### **Commentary**:\n",
        "When evaluating model metrics, a rule of thumb is, what does 0 mean, what does 1 mean? The way I go about it is literally to start at Google when I'm unsure. Something like:\n",
        "\n",
        "**F1_Score**: This is the most [important](https://wiki.pathmind.com/accuracy-precision-recall-f1) metric IMO.\n",
        "\n",
        "F1 is an overall measure of a model’s accuracy that combines *precision* and *recall*.\n",
        "\n",
        "An F1 score is considered perfect when it’s 1, while the model is a total failure when it’s 0. In our model results, we have an F1 score of **.65 ** or **65%** which is not a total failure or a total success.\n",
        "\n",
        "**Precision**:\n",
        "The percentage of predictions that were **correct (positive)**. The higher the precision, the fewer false positives predicted.\n",
        "\n",
        "Here the model identified the `income_bracket` with a precision score of **0.58**, which meant that this model was correct *58%* of the time.\n",
        "\n",
        "**Recall**:\n",
        "\n",
        "The percentage of all ground truth items that were successfully predicted by the model.\n",
        "\n",
        "The higher the recall, the fewer false negatives, or the fewer predictions missed.\n",
        "\n",
        "Here the model identified the `income_bracket` recall score of **0.75** which means the model correctly classified 75% of all the census respondent buckets in the test data\n",
        "\n",
        "**Accuracy**: *How often is our model correct?*\n",
        "- [What does a good accuracy score look like?](https://www.google.com/search?q=What+is+a+good+AUC+ROC+score%3F&ei=-f-UY-T3HYuNggff94GwCg&ved=0ahUKEwik2PL2hPD7AhWLhuAKHd97AKYQ4dUDCBA&uact=5&oq=What+is+a+good+AUC+ROC+score%3F&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIECAAQHjIFCAAQhgMyBQgAEIYDMgUIABCGAzIFCAAQhgMyBQgAEIYDOgcIABAeELADOggIABCGAxCwAzoGCAAQFhAeSgQIQRgBSgQIRhgAUFpY5wlggQxoAXAAeACAAWGIAeMBkgEBM5gBAKABAcgBBsABAQ&sclient=gws-wiz-serp) subjective but anything greater than 70% is a great model performance.\n",
        "\n",
        "Accuracy is the fraction of predictions our model got right. This is the # of correct predictions (income_bracket correctly classified) over the Total # of predictions.\n",
        "\n",
        "Here the model identified the `income_bracket` accuracy score of **0.80** which means the model correctly classified 80% of all the census respondent buckets in the test data\n",
        "\n",
        "**Log_loss**:\n",
        "[Log-loss](https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a#:~:text=Log%2Dloss%20is%20indicative%20of,is%20the%20log%2Dloss%20value) is indicative of how close the prediction probability is to the corresponding actual/true value (<=50K or >=50k for income_bracket). The more the predicted probability diverges from the actual value, the higher is the log-loss value.\n",
        "\n",
        "Here the model has a log loss of **0.40** which means the prediction probability is close to the actual label when compared to the training data. The lower the log loss, the better the model.\n",
        "\n",
        "**ROC_AUC**:\n",
        "- [What does a good AUC ROC score look like?](https://www.google.com/search?q=What+is+a+good+AUC+ROC+score%3F&ei=-f-UY-T3HYuNggff94GwCg&ved=0ahUKEwik2PL2hPD7AhWLhuAKHd97AKYQ4dUDCBA&uact=5&oq=What+is+a+good+AUC+ROC+score%3F&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIECAAQHjIFCAAQhgMyBQgAEIYDMgUIABCGAzIFCAAQhgMyBQgAEIYDOgcIABAeELADOggIABCGAxCwAzoGCAAQFhAeSgQIQRgBSgQIRhgAUFpY5wlggQxoAXAAeACAAWGIAeMBkgEBM5gBAKABAcgBBsABAQ&sclient=gws-wiz-serp) Answer is between 0.8-0.9.\n",
        "\n",
        "AUC - ROC curve is a performance measurement for the classification problems at various threshold settings.\n",
        "\n",
        "ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes.\n",
        "\n",
        "![AUC ROC](https://stephenallwright.com/content/images/2022/08/roc-auc-curve-diagram.png?ezimgfmt=rs:466x466/rscb1/ng:webp/ngcb1)\n",
        "\n",
        "#### **Questions**:\n",
        "- Did our selection of the features we used for training affect these model evaluations?\n",
        "\n",
        "\n",
        "There are various metrics for logistic regression and other model types (full list of metrics can be found in the <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate#mlevaluate_output\" target=\"_blank\">documentation</a>)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Vo2Xyupcmd"
      },
      "source": [
        "### Step 5: Use the ML.PREDICT function to predict the income bracket for a given set of census participants.\n",
        "\n",
        "Finally, you use the [ML.PREDICT](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict) function to predict the income bracket for a given set of census participants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuWWFTEDDv-O"
      },
      "outputs": [],
      "source": [
        "# Recap of what's in our view data.\n",
        "\n",
        "# We can examine the view data using the sql below.\n",
        "sql_examine_view_data = f\"\"\"\n",
        "SELECT * FROM {BQ_DATASET_NAME}.input_view WHERE dataframe='prediction';\n",
        "\"\"\"\n",
        "\n",
        "run_bq_query(sql_examine_view_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m9FCEEFpdPq"
      },
      "outputs": [],
      "source": [
        "# We are using the remainder of our data ot perform predictions by limiting the dataframe to \"PREDICTION\". About 3142 rows.\n",
        "\n",
        "sql_ml_predict = f\"\"\"\n",
        "SELECT * FROM\n",
        " ML.PREDICT (MODEL {BQ_DATASET_NAME}.{BQML_MODEL_NAME},\n",
        "   (SELECT income_bracket AS original_income_bracket,age,workclass,marital_status,education_num,occupation,hours_per_week,dataframe FROM {BQ_DATASET_NAME}.input_view\n",
        "   WHERE dataframe = 'prediction')\n",
        " ) LIMIT 5\n",
        "\"\"\"\n",
        "print(sql_ml_predict)\n",
        "\n",
        "run_bq_query(sql_ml_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1odXkGxtFMwP"
      },
      "source": [
        "\n",
        "#### **Commentary**:\n",
        "When the ML predict function runs, it generates 2 new columns. These are:\n",
        "1. predicted_income_bracket\n",
        "1. predicted_income_bracket_probs\n",
        "\n",
        "The `predicted_income_bracket` has the classfication predictions of the original `income_bracket` label.\n",
        "\n",
        "The `predicted_income_bracket_probs` contains the probability scores for each of the binary options.\n",
        "\n",
        "Lets take a look at **row 3 **who's prediction results are as follows:\n",
        "```\n",
        "predicted_income_bracket = <=50K\n",
        "predicted_income_bracket_probs\t=\n",
        "{'label': ' >50K', 'prob': 0.011691176874960766}\n",
        "{'label': ' <=50K','prob': 0.9883088231250392}\n",
        "income_bracket = <=50k,\n",
        "```\n",
        "The model's income_bracket probability was 1% for >50K, and 98% for <=50k based on the inputs provided. This matched the dataset original label.\n",
        "\n",
        "```\n",
        "age = 73,\n",
        "workclass=Private,\n",
        "marital_status=Widowed,\n",
        "education_num=5,\n",
        "occupation=Other-service\n",
        "hours_per_week=0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e806ebc48a2"
      },
      "source": [
        "### Step 6: Use the ML.EXPLAIN_PREDICT function to know which features are the most important to determine the weight\n",
        "\n",
        "To understand why the model is generating these prediction results, you can use the ML.EXPLAIN_PREDICT function.\n",
        "\n",
        "\n",
        "<a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-explain-predict\" target=\"_blank\">ML.EXPLAIN_PREDICT</a> has built-in <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-xai-overview\" target=\"_blank\">Explainable AI</a>. This allows you to see the top contributing features to each prediction and interpret how it was computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d500fdbfb44"
      },
      "outputs": [],
      "source": [
        "sql_explain_predict = f\"\"\"\n",
        "\n",
        "SELECT * FROM\n",
        "ML.EXPLAIN_PREDICT(MODEL {BQ_DATASET_NAME}.{BQML_MODEL_NAME},\n",
        " (\n",
        "  SELECT income_bracket AS original_income_bracket,age,workclass,marital_status,education_num,occupation,hours_per_week,dataframe FROM {BQ_DATASET_NAME}.input_view\n",
        "  WHERE dataframe = 'prediction'),\n",
        "  STRUCT(3 as top_k_features)\n",
        " ) LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "run_bq_query(sql_explain_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGakM2WBKYlg"
      },
      "source": [
        "\n",
        "#### **Commentary**:\n",
        "![AUC ROC](https://github.com/paulycloud/ml_portfolio/blob/main/02_BigQuery_ML/02_census_logistic_reg/assets/ml_explain_predict.png?raw=true)\n",
        "\n",
        "When the ML.EXPLAIN_PREDICT function runs, it generates 6 new columns. These are:\n",
        "1. predicted_income_bracket\n",
        "1. probability\n",
        "1. top_feature_attributions\n",
        "1. baseline_prediction_value\n",
        "1. prediction_value\n",
        "1. approximation_error\n",
        "\n",
        "Lets take a look at **row 3** who's prediction results are as follows:\n",
        "\n",
        "```\n",
        "predicted_income_bracket = <=50K\n",
        "probability=0.9883088231250392\n",
        "top_feature_attributions= ['education_num', 'marital_status','age']\n",
        "baseline_prediction_value=-0.29878674958313467\n",
        "prediction_value=-4.437160778853922\n",
        "approximation_error=0.0\n",
        "\n",
        "age = 73,\n",
        "workclass=Private,\n",
        "marital_status=Widowed,\n",
        "education_num=5,\n",
        "occupation=Other-service\n",
        "hours_per_week=0\n",
        "```\n",
        "\n",
        "The `predicted_income_bracket` has the classfication predictions of the original `income_bracket` label. In this case, its <=50K\n",
        "\n",
        "The `probability` contains the probability scores for each of the binary options in this case its 98%.\n",
        "\n",
        "The `top_feature_attributions` this is an array containing the 3 top features contributng to the prediction. In this case, Education_num, marital_status and age are the top factors.\n",
        "\n",
        "The `baseline_prediction_value` is the mean across all numerical features and NULL for other types of features.\n",
        "\n",
        "The `prediction_value` contains the probability scores for each of the binary options.\n",
        "\n",
        "The `approximation_error` The exact attribution methods like Tree SHAP have the property such that baseline_prediction_value + {sigma} feature_attribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc0c1c1b03f9"
      },
      "source": [
        "### Step 7: Inspect the model on Vertex AI Model Registry\n",
        "\n",
        "When the model was trained in BigQuery ML, the line `model_registry=\"vertex_ai\"` registered the model to Vertex AI Model Registry automatically upon completion.\n",
        "\n",
        "You can view the model on the <a href=\"https://console.cloud.google.com/vertex-ai/models\" target=\"_blank\">Vertex AI Model Registry page</a>, or use the code below to check that it was successfully registered:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b00664302839"
      },
      "outputs": [],
      "source": [
        "model = vertex_ai.Model(model_name=BQML_MODEL_NAME)\n",
        "print(f'Review the Model in the Console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{BQML_MODEL_NAME}?project={PROJECT_ID}')\n",
        "\n",
        "print(model.gca_resource)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89455f708f54"
      },
      "source": [
        "### Step 8: Create the endpoint and Deploy the model to an endpoint\n",
        "Endpoints are machine learning models made available for online prediction requests. Endpoints are useful for timely predictions from many users (for example, in response to an application request). You can also request batch predictions if you don't need immediate results.\n",
        "\n",
        "While BigQuery ML supports batch prediction with <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict\" target=\"_blank\">ML.PREDICT</a> and <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-explain-predict\" target=\"_blank\">ML.EXPLAIN_PREDICT</a>, BigQuery ML is not suitable for real-time predictions where you need low latency predictions with potentially high frequency of requests.\n",
        "\n",
        "In other words, deploying the BigQuery ML model to an endpoint enables you to do online predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ab7e1ac83c"
      },
      "source": [
        "#### Create a Vertex AI endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a61ea55f685"
      },
      "source": [
        "To deploy your model to an endpoint, you will first need to create an endpoint before you deploy the model to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7d80941cf30"
      },
      "outputs": [],
      "source": [
        "# create an endpoint if it doesn't exist\n",
        "endpoints = vertex_ai.Endpoint.list(filter = f\"display_name={BQML_ENDPOINT_NAME}\")\n",
        "\n",
        "if endpoints:\n",
        "    endpoint = endpoints[0]\n",
        "    print(f\"Endpoint Exists: {endpoints[0].display_name}\")\n",
        "else:\n",
        "    endpoint = vertex_ai.Endpoint.create(\n",
        "    display_name=BQML_ENDPOINT_NAME,\n",
        "    location=REGION,\n",
        "    project=PROJECT_ID,\n",
        "    )\n",
        "    print(f\"Endpoint Created: {endpoint.display_name}\")\n",
        "\n",
        "print(f'Review the Endpoint in the Console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/endpoints/{endpoint.name}?project={PROJECT_ID}')\n",
        "\n",
        "print(endpoint.display_name)\n",
        "print(endpoint.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a90be5b77a2"
      },
      "source": [
        "#### Deploy model to Vertex endpoint\n",
        "\n",
        "With the new endpoint, you can now deploy your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99a7rwSiV3c0"
      },
      "outputs": [],
      "source": [
        "# deploying the model to the endpoint may take 10-15 minutes\n",
        "# Check if Model Deployment Exists\n",
        "deployments = vertex_ai.Endpoint.list(filter = f\"display_name={BQML_ENDPOINT_NAME}\")\n",
        "\n",
        "if deployments:\n",
        "    deployment = deployments[0]\n",
        "    print(f\"Deployment Exists: {deployments[0].display_name}\")\n",
        "else:\n",
        "    model.deploy(endpoint=endpoint)\n",
        "    print(f\"Model deployment Created: {endpoint.display_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c303d779477b"
      },
      "source": [
        "You can also check on the status of your model by visiting the <a href=\"https://console.cloud.google.com/vertex-ai/endpoints\" target=\"_blank\">Vertex AI Endpoints page</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a888784de7b"
      },
      "source": [
        "### Step 9: Make online predictions to the endpoint\n",
        "\n",
        "Using a sample of the training data, you can test the endpoint to make online predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f17b049a846c"
      },
      "outputs": [],
      "source": [
        "df_sample_requests_list = [\n",
        "    {\n",
        "        \"age\": 45,\n",
        "        \"workclass\": \"Public\",\n",
        "        \"marital_status\": \"Single\",\n",
        "        \"education_num\": 6,\n",
        "        \"occupation\": \"Exec-managerial\",\n",
        "        \"hours_per_week\": 40,\n",
        "    },\n",
        "    {\n",
        "        \"age\": 30,\n",
        "        \"workclass\": \"Private\",\n",
        "        \"marital_status\": \"Married\",\n",
        "        \"education_num\": 2,\n",
        "        \"occupation\": \"Machine-op-inspct\",\n",
        "        \"hours_per_week\": 50,\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4839f31d2f8"
      },
      "outputs": [],
      "source": [
        "prediction = endpoint.predict(df_sample_requests_list)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "347ffda86396"
      },
      "source": [
        "You can then extract the predictions from the prediction response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea659b7c178d"
      },
      "outputs": [],
      "source": [
        "# to view the prediction for the first row\n",
        "prediction.predictions[1]['predicted_income_bracket']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can <a href=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects\" target=\"_blank\">delete the Google Cloud\n",
        "project</a> you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Undeploy model from endpoint and delete endpoint\n",
        "endpoint.undeploy_all()\n",
        "endpoint.delete()\n",
        "\n",
        "# Delete BigQuery dataset, including the BigQuery ML model\n",
        "! bq rm -r -f $PROJECT_ID:$BQ_DATASET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oN15xrZSnZHM",
        "1odXkGxtFMwP",
        "nGakM2WBKYlg"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}