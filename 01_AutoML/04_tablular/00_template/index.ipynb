{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "#  Predict whether a Bank client will purchase a Term deposit (1=no, 2=yes) using AutoML Binary Logistic Regression\n",
        "\n",
        "###### Copyright \n",
        "**GUI Author:** [ Google Cloud](https://cloud.google.com/vertex-ai/docs/tutorials/tabular-automl/overview)<br>\n",
        "\n",
        "**Notebook Author:** [Paul Kamau](https://paulkamau.com)<br>\n",
        "\n",
        "**Project Type:** AutoML Tabular<br>\n",
        "\n",
        "**Date created:** 2022/04/16<br>\n",
        "\n",
        "**Last modified:** 2022/04/16<br>\n",
        "\n",
        "**Description:** The goal of the trained model is to predict whether a bank client will buy a term deposit (a type of investment) using features like age, income, and profession. This type of model can help banks determine who to focus its marketing resources on.\n",
        "\n",
        "**Training Dataset** This project uses the [Bank marketing](https://datahub.io/machine-learning/bank-marketing) open-source dataset. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411c6c769293"
      },
      "source": [
        "### Objective\n",
        "\n",
        "The steps performed include the following:\n",
        "\n",
        "- Create a Vertex AI model training job.\n",
        "- Train an AutoML Tabular model.\n",
        "- Deploy the `Model` resource to a serving `Endpoint` resource.\n",
        "- Make a prediction by sending data.\n",
        "- Undeploy the `Model` resource.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "This project uses the [Bank marketing](https://datahub.io/machine-learning/bank-marketing) open-source dataset. \n",
        "\n",
        "```https://storage.googleapis.com/cloud-ml-tables-data/bank-marketing.csv```\n",
        "\n",
        "#### Execute notebook in Colab\n",
        "<a href=\"#\">\n",
        "    <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs_AFFK5Qd_S"
      },
      "source": [
        "# Project Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH7HQzDJQgC0",
        "outputId": "f290b209-7867-40e1-81ca-e816f0ef7b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All project variables set. Lets go\n"
          ]
        }
      ],
      "source": [
        "# Project variables \n",
        "#\n",
        "# These are the project variable used in this ML Model: \n",
        "#\n",
        "\n",
        "PROJECT_ID = \"paulkamau\" # @param {type:\"string\"}\n",
        "REGION = \"us-central1\" #@param [\"us-central1\", \"europe-west4\", \"asia-east1\", \"video\"]\n",
        "automl_type = \"tabular\" #@param [\"image\", \"text\", \"tabular\", \"video\"]\n",
        "model_display_name = \"automl_tabular_bank_purchase_model\"\n",
        "job_display_name = model_display_name + \"_job\"\n",
        "batch_display_name = model_display_name + \"_batch_prediction\"\n",
        "endpoint_display_name = model_display_name + \"_endpoint\"\n",
        "\n",
        "## datasets \n",
        "dataset_display_name = model_display_name + \"_dataset\"\n",
        "dataset_source_uri = \"gs://cloud-ml-tables-data/bank-marketing.csv\"\n",
        "dataset_local_source_uri = \"gs://auto-ml-tutorials/tabular/bank-marketing.csv\"\n",
        "dataset_source_public = \"https://storage.googleapis.com/cloud-ml-tables-data/bank-marketing.csv\"\n",
        "\n",
        "# bucket details\n",
        "BUCKET_NAME = \"auto-ml-tutorials\" #auto-ml bucket \n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}/{automl_type}/\" # automl bucket uri\n",
        "BUCKET_PREDICTION_OUTPUT = f\"gs://auto_ml_datasets_predictions/{automl_type}/\" \n",
        "BUCKET_INPUT_BATCHPREDICT = f\"gs://{BUCKET_NAME}/{automl_type}/{model_display_name}\" # contains the files to be used for batch prediction\n",
        "\n",
        "print(\"All project variables set. Lets go\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b03b7f4487ff"
      },
      "source": [
        "Install the latest version of the Vertex AI client library.\n",
        "\n",
        "Run the following command in your virtual environment to install the Vertex SDK for Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d489d38261dd"
      },
      "outputs": [],
      "source": [
        "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_storage"
      },
      "source": [
        "Install the Cloud Storage library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qssss-KSlugo"
      },
      "outputs": [],
      "source": [
        "!pip install {USER_FLAG} --upgrade google-cloud-storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3a26cb9b19d"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Notebooks**, your environment is already\n",
        "authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "535223fa4b84"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "This notebook demonstrates how to use Vertex AI SDK for Python to create an AutoML model based on a tabular dataset. You will need to provide a Cloud Storage bucket where the dataset will be stored.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all of your Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhOb7YnwClBb",
        "outputId": "65e69de0-1918-4cae-9f7b-8c51f8a09d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         0  2022-07-27T17:51:07Z  gs://auto-ml-tutorials/tabular/#1658944267893591  metageneration=1\n",
            "   3700818  2022-07-27T20:05:40Z  gs://auto-ml-tutorials/tabular/bank-marketing.csv#1658952340977817  metageneration=1\n",
            "TOTAL: 2 objects, 3700818 bytes (3.53 MiB)\n"
          ]
        }
      ],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Uo3tifg1kx"
      },
      "source": [
        "### Import Vertex SDK for Python\n",
        "\n",
        "Import the Vertex SDK into your Python environment and initialize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "643dfd86b00d"
      },
      "source": [
        "## Build the Bank Deposit AutoML Model \n",
        "\n",
        "This will be a classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSAIAl-ccnfd"
      },
      "source": [
        "## Read The Dataset\n",
        "\n",
        "This code will display a sample of the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA94DY6UckF4",
        "outputId": "16caca39-d2a3-4be7-a464-fb40ac157116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5 rows sample\n",
            "   Age           Job MaritalStatus  Education Default  Balance Housing Loan  Contact  Day Month  Duration  Campaign  PDays  Previous POutcome  Deposit\n",
            "0   58    management       married   tertiary      no     2143     yes   no  unknown    5   may       261         1     -1         0  unknown        1\n",
            "1   44    technician        single  secondary      no       29     yes   no  unknown    5   may       151         1     -1         0  unknown        1\n",
            "2   33  entrepreneur       married  secondary      no        2     yes  yes  unknown    5   may        76         1     -1         0  unknown        1\n",
            "3   47   blue-collar       married    unknown      no     1506     yes   no  unknown    5   may        92         1     -1         0  unknown        1\n",
            "4   33       unknown        single    unknown      no        1      no   no  unknown    5   may       198         1     -1         0  unknown        1\n",
            "\n",
            "These are the columns\n",
            "Age\n",
            "Job\n",
            "MaritalStatus\n",
            "Education\n",
            "Default\n",
            "Balance\n",
            "Housing\n",
            "Loan\n",
            "Contact\n",
            "Day\n",
            "Month\n",
            "Duration\n",
            "Campaign\n",
            "PDays\n",
            "Previous\n",
            "POutcome\n",
            "Deposit\n"
          ]
        }
      ],
      "source": [
        "# Read the data\n",
        "dataset_sample = pd.read_csv(dataset_source_public,nrows=5)\n",
        "                                   \n",
        "# Use the next code cell to print the first five rows of the data.\n",
        "dataset_sample.head()\n",
        "\n",
        "print(\"\\n5 rows sample\")\n",
        "print(dataset_sample.to_string())\n",
        "\n",
        "## list columns \n",
        "print(\"\\nThese are the columns\")\n",
        "cols = list(pd.read_csv(dataset_source_public, nrows =1))\n",
        "for i in cols:\n",
        "       print(i)\n",
        "\n",
        "# Shape of training data (num_rows, num_columns)\n",
        "#print(dataset_sample.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f4f50a0112c"
      },
      "source": [
        "### Create a Managed Tabular Dataset from a CSV\n",
        "\n",
        "This section will create a dataset from a CSV file stored on your GCS bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gecvE_bmXQzX",
        "outputId": "ba7a5878-47b3-4863-c33e-1997ffffa212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Exists: automl_tabular_bank_purchase_model_dataset\n"
          ]
        }
      ],
      "source": [
        "# Create dataset if it doesn't exist\n",
        "datasets = aiplatform.TabularDataset.list(filter = f\"display_name={dataset_display_name}\")\n",
        "\n",
        "if datasets:\n",
        "    dataset = datasets[0]\n",
        "    print(f\"Dataset Exists: {dataset.display_name}\")\n",
        "else:\n",
        "    #create the new dataset \n",
        "    dataset = aiplatform.TabularDataset.create (\n",
        "    display_name=dataset_display_name,\n",
        "    gcs_source=dataset_local_source_uri,\n",
        "    )\n",
        "    print(f\"Dataset Created: {dataset.display_name}\")\n",
        "    \n",
        "    print(f'Review the Dataset in the Console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/datasets/{dataset.resource_name}?project={PROJECT_ID}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba5011d50ac7"
      },
      "source": [
        "### Launch a Training Job to Create a Model\n",
        "\n",
        "You create a custom model by training it using a prepared dataset. AutoML Tables uses the items from the dataset to train the model, test it, and evaluate its performance. You can review the results, adjust the training dataset as needed and train a new model using the improved dataset.\n",
        "\n",
        "[SDK docs reference](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.AutoMLTabularTrainingJob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24c2c081d683",
        "outputId": "2f9a32df-7e4d-48ec-d4c5-fe1dd63b11a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jobs Exists: automl_tabular_bank_purchase_model_job\n"
          ]
        }
      ],
      "source": [
        "# Define the training job and create one if it doesn't exit \n",
        "jobs = aiplatform.AutoMLTabularTrainingJob.list(filter = f\"display_name={job_display_name}\")\n",
        "\n",
        "if jobs:\n",
        "   job = jobs[0]\n",
        "   print(f\"Jobs Exists: {jobs[0].display_name}\")\n",
        "else:\n",
        "    job = aiplatform.AutoMLTabularTrainingJob(\n",
        "    display_name=job_display_name,\n",
        "    optimization_prediction_type=\"classification\",\n",
        "    )\n",
        "    print(\"Training job prepared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOyZTv10X3Jw"
      },
      "source": [
        "### Launch a Model run job\n",
        "\n",
        "Once we have defined your training script, we will create a model. The `run` function creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` function returns the `Model` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd4zTfHWX3mp",
        "outputId": "5930e648-03cf-49e2-fe05-429a9fc16f49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Exists: automl_tabular_bank_purchase_model\n"
          ]
        }
      ],
      "source": [
        "# Create model job if it doesn't exist\n",
        "models = aiplatform.Model.list(filter = f\"display_name={model_display_name}\")\n",
        "\n",
        "if models:\n",
        "    model = models[0]\n",
        "    print(f\"Model Exists: {models[0].display_name}\")\n",
        "else:\n",
        "    model = job.run(\n",
        "    dataset=dataset,\n",
        "    target_column=\"Deposit\",\n",
        "    model_display_name=model_display_name,\n",
        "    training_fraction_split=0.8,\n",
        "    validation_fraction_split=0.1,\n",
        "    test_fraction_split=0.1,\n",
        "    sync=True,\n",
        "    )\n",
        "    print(f\"Model Created: {model.display_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCEHwQI8N1da"
      },
      "source": [
        "# Review model evaluation scores\n",
        "After your model training has finished, you can review the evaluation scores for it using the list_model_evaluations() method. This method will return an iterator for each evaluation slice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di9PkII9N3qw",
        "outputId": "2986d1e3-a590-45bd-9c0d-da9031659ab5"
      },
      "outputs": [],
      "source": [
        "# Model Evaluations\n",
        "model_evaluations = model.list_model_evaluations()\n",
        "model_evaluation = list(model_evaluations)[0]\n",
        "print(model_evaluation)\n",
        "\n",
        "\n",
        "# Print the evaluation metrics\n",
        "for evaluation in model_evaluations:\n",
        "    evaluation = evaluation.to_dict()\n",
        "    print(\"Model's evaluation metrics from Training:\\n\")\n",
        "    metrics = evaluation[\"metrics\"]\n",
        "    for metric in metrics.keys():\n",
        "        print(f\"metric: {metric}, value: {metrics[metric]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93a4d034c73b",
        "tags": []
      },
      "source": [
        "# Endpoints (Create Endpoint, Deploy Model, Make Prediction)\n",
        "Endpoints are machine learning models made available for online prediction requests. Endpoints are useful for timely predictions from many users (for example, in response to an application request). You can also request batch predictions if you don't need immediate results.\n",
        "\n",
        "To create an endpoint, you need at least one machine learning model. [SDK Docs](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-console?_ga=2.2361459.-270543887.1669322993)\n",
        "\n",
        "### Deploy your model\n",
        "\n",
        "Before you use your model to make predictions, you need to deploy it to an `Endpoint`. You can do this by calling the `deploy` function on the `Model` resource. This function does two things:\n",
        "\n",
        "1. Creates an `Endpoint` resource to which the `Model` resource will be deployed.\n",
        "\n",
        "2. Deploys the `Model` resource to the `Endpoint` resource.\n",
        "\n",
        "Deploy your model.\n",
        "\n",
        "### NOTE: Wait until the model **FINISHES** deployment before proceeding to prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSoN1qu3HMCY"
      },
      "source": [
        "## Create the Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulkrNwbVwabq",
        "outputId": "18e66b5a-0b2a-488f-c504-5b7bcaf6875d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Endpoint Exists: automl_tabular_bank_purchase_model_endpoint\n",
            "Review the Endpoint in the Console:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/endpoints/5738047720153677824?project=paulkamau\n"
          ]
        }
      ],
      "source": [
        "# Check if Endpoint Exists\n",
        "endpoints = aiplatform.Endpoint.list(filter = f\"display_name={endpoint_display_name}\")\n",
        "\n",
        "if endpoints:\n",
        "    endpoint = endpoints[0]\n",
        "    print(f\"Endpoint Exists: {endpoints[0].display_name}\")\n",
        "else:\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "    display_name=endpoint_display_name,\n",
        "    )\n",
        "    print(f\"Endpoint Created: {endpoint.display_name}\")\n",
        "\n",
        "print(f'Review the Endpoint in the Console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/endpoints/{endpoint.name}?project={PROJECT_ID}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwLCgvw7HPJy"
      },
      "source": [
        "## Deploy the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtPSes4EG6vf"
      },
      "outputs": [],
      "source": [
        "# deploying the model to the endpoint may take 10-15 minutes\n",
        "\n",
        "# Check if Model Deployment Exists\n",
        "deployments = endpoint.list(filter = f\"display_name={model_display_name}\")\n",
        "\n",
        "if deployments:\n",
        "    deployment = deployments[0]\n",
        "    print(f\"Deployment Exists: {deployments[0].display_name}\")\n",
        "else:\n",
        "    deployment = endpoint.deploy(\n",
        "    model=model,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        "    machine_type='n1-standard-4',\n",
        "    accelerator_type='NVIDIA_TESLA_K80',\n",
        "    accelerator_count=1\n",
        "    )\n",
        "    print(f\"Model deployment Created: {endpoint.display_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd44380b9ae3"
      },
      "source": [
        "## Predict on the endpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13f3e8aa27c0"
      },
      "source": [
        "* This sample instance is taken from an observation in which `Deposit` = **1**\n",
        "* Note that the values are all strings. Since the original data was in CSV format, everything is treated as a string. The transformations you defined when creating your `AutoMLTabularTrainingJob` inform Vertex AI to transform the inputs to their defined types.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1tzme-mMgER"
      },
      "source": [
        "#### Quick of what the training dataset looks like "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9dpikKeMfcf",
        "outputId": "52894ee3-755b-410b-a209-3011e8846f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5 rows sample\n",
            "   Age           Job MaritalStatus  Education Default  Balance Housing Loan  Contact  Day Month  Duration  Campaign  PDays  Previous POutcome  Deposit\n",
            "0   58    management       married   tertiary      no     2143     yes   no  unknown    5   may       261         1     -1         0  unknown        1\n",
            "1   44    technician        single  secondary      no       29     yes   no  unknown    5   may       151         1     -1         0  unknown        1\n",
            "2   33  entrepreneur       married  secondary      no        2     yes  yes  unknown    5   may        76         1     -1         0  unknown        1\n",
            "\n",
            "These are the columns\n",
            "Age\n",
            "Job\n",
            "MaritalStatus\n",
            "Education\n",
            "Default\n",
            "Balance\n",
            "Housing\n",
            "Loan\n",
            "Contact\n",
            "Day\n",
            "Month\n",
            "Duration\n",
            "Campaign\n",
            "PDays\n",
            "Previous\n",
            "POutcome\n",
            "Deposit\n"
          ]
        }
      ],
      "source": [
        "# Read the data\n",
        "dataset_sample = pd.read_csv(dataset_source_public,nrows=3)\n",
        "                                  \n",
        "# Use the next code cell to print the first five rows of the data.\n",
        "dataset_sample.head()\n",
        "\n",
        "print(\"\\n5 rows sample\")\n",
        "print(dataset_sample.to_string())\n",
        "\n",
        "## list columns \n",
        "print(\"\\nThese are the columns\")\n",
        "cols = list(pd.read_csv(dataset_source_public, nrows =1))\n",
        "for i in cols:\n",
        "       print(i)\n",
        "\n",
        "# Shape of training data (num_rows, num_columns)\n",
        "#print(dataset_sample.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndjOmJ7xMkUO"
      },
      "source": [
        "### Create new predictions with the model\n",
        "\n",
        "We are attempting to predict the \"Deposit\" Value. Either 1 or 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "oZiGczeR3GfI"
      },
      "outputs": [],
      "source": [
        "df_sample_requests_list = [\n",
        "    {\n",
        "      \"Age\": \"33\",\n",
        "      \"Job\":\"Technician\",\n",
        "      \"MaritalStatus\":\"single\",\n",
        "      \"Education\":\"secondary\",\n",
        "      \"Default\":\"no\",\n",
        "      \"Balance\": \"3000\",\n",
        "      \"Housing\":\"yes\",\n",
        "      \"Loan\":\"No\",\n",
        "      \"Contact\":\"unknown\",\n",
        "      \"Day\": \"7\",\n",
        "      \"Month\":\"June\",\n",
        "      \"Duration\":\"200\",\n",
        "      \"Campaign\":\"1\",\n",
        "      \"PDays\":\"-1\",\n",
        "      \"Previous\":\"0\",\n",
        "      \"POutcome\":\"unknown\",\n",
        "   }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00c0d01dc8ae",
        "outputId": "ea41eafb-e492-49ab-fdae-f54b14207065",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'scores': [0.997821033000946, 0.002178938593715429], 'classes': ['1', '2']}]\n"
          ]
        }
      ],
      "source": [
        "prediction = endpoint.predict(df_sample_requests_list)\n",
        "\n",
        "print(prediction.predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "154258dfb12f"
      },
      "source": [
        "### Undeploy the model\n",
        "\n",
        "To undeploy your `Model` resource from the serving `Endpoint` resource, use the endpoint's `undeploy` method with the following parameter:\n",
        "\n",
        "- `deployed_model_id`: The model deployment identifier returned by the prediction service when the `Model` resource is deployed. You can retrieve the `deployed_model_id` using the prediction object's `deployed_model_id` property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "186856f896fc"
      },
      "outputs": [],
      "source": [
        "endpoint.undeploy(deployed_model_id=prediction.deployed_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7d2aa967f46",
        "tags": []
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Training Job\n",
        "- Model\n",
        "- Endpoint\n",
        "- Cloud Storage Bucket\n",
        "\n",
        "**Note**: You must delete any `Model` resources deployed to the `Endpoint` resource before deleting the `Endpoint` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a9c201f8589"
      },
      "outputs": [],
      "source": [
        "delete_training_job = True\n",
        "delete_model = True\n",
        "delete_endpoint = True\n",
        "\n",
        "# Warning: Setting this to true will delete everything in your bucket\n",
        "delete_bucket = False\n",
        "\n",
        "# Delete the training job\n",
        "job.delete()\n",
        "\n",
        "# Delete the model\n",
        "model.delete()\n",
        "\n",
        "# Delete the endpoint\n",
        "endpoint.delete()\n",
        "\n",
        "if delete_bucket and \"BUCKET_NAME\" in globals():\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-6.m98",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m98"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
